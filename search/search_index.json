{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Data_Engineering_Analysis/","title":"Data Engineering and analysis","text":""},{"location":"Data_Engineering_Analysis/Architecture/","title":"Architecture","text":""},{"location":"Data_Engineering_Analysis/Architecture/idempotence/","title":"Idempotence","text":"<p>https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/CTE_recursion/","title":"CTE recursion","text":"<p>Warning</p> <p>CTE recursion is not real recursion, it is just iteration.</p> <p>With Common Table Expressions (CTE)  you can create recursions. An often used example is to traverse an organisational hierarchy. Let's say you have a table containing employees and who their manager is. Then if you provide a starting point it can traverse the hierarchy tree by joining the <code>MANAGER_ID</code> with an <code>EMPLOYEE_ID</code>.</p> <p>Explaining how to understand this recursion functionally is much easier than explaining how the  recursion actually technically happens. So first we will discuss it functionally and then technically.</p> <p>But first, let's define a table to work with. To keep it simple we will use a slightly modified version of the organisational hierarchy examples. In many examples the <code>MANAGER_ID</code> of the person at the very top is <code>NULL</code> as he/she does not have a manager. However, we will make it a bit more difficult by  making that person the manager of itself.</p> <pre><code>CREATE TABLE #Hierarchy\n(\n    ID INT\n,   EMP_NAME VARCHAR(20)\n,   MANAGER_ID INT\n)\n\nINSERT INTO #Hierarchy\n(ID, EMP_NAME, MANAGER_ID)\nVALUES\n(1,     'Alice' ,   1),\n(2,     'Bob'   ,   1),\n(3,     'Charlie',  1),\n(4,     'Donnie',   3),\n(5,     'Erica',    4)\n</code></pre> <p></p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/CTE_recursion/#functional-explanation","title":"Functional explanation","text":"<p>The CTE format for recursion consists of 2 parts:</p> <ol> <li>The anchor query</li> <li>The query used to fetch the next rows</li> </ol> <p>The anchor query is your starting point, it determines in our example at which employee to start the recursion with. The other query will join your base table with the previous results from the CTE.</p> <pre><code>WITH CTE_recursion AS\n(\n    SELECT *\n    FROM BASE_TABLE\n    WHERE &lt;filter for your starting point&gt;\n\n    UNION ALL\n\n    SELECT B.*\n    FROM BASE_TABLE AS B\n        JOIN CTE_recursion AS R\n        ON B.&lt;some_column&gt; = R.&lt;Some_column&gt;\n)\nSELECT *\nFROM CTE_recursion\n</code></pre> <p>In the second part of the CTE the <code>BASE_TABLE</code> is the table providing the new rows and the join to the <code>CTE_recursion</code> (itself) represents the rows of the previous step. So in the organisational  hierarchy example, if you want to know who the manager is, the CTE contains the employee info and the <code>BASE_TABLE</code> will provide the information for the manager. In the next loop this manager is then  considered as the employee and the corresponding manager will be looked up. This goes on until the second part of the CTE does not give any rows. </p> <p>Warning</p> <p>If there is a circular reference, the loop will go on until it reaches the maximum recursions. The default is 100. If you think you need more or less tries, you can put the following option at the end of your query <code>OPTION (MAXRECURSION 2)</code>, in this case it only goes 2 layers deep.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/CTE_recursion/#example-going-down-the-tree","title":"Example going down the tree","text":"<p>Let's revisit our own example. </p> <p></p> <p>Say we are <code>Donnie</code> and we want to know who he is the manager of, and who that person is the manager of  etc. Since Donnie is only manager of Erica, we expect 2 records since the anchor row will always be there. The SQL query for that is shown below.</p> <pre><code>WITH CTE_Recursive AS\n(\n    SELECT *\n    FROM #Hierarchy\n    WHERE ID = 4\n\n    UNION ALL\n\n    SELECT H.*\n    FROM #Hierarchy AS H\n        JOIN CTE_Recursive AS R\n            ON H.MANAGER_ID = R.ID\n)\nSELECT *\nFROM CTE_Recursive\n</code></pre> <p>We start by defining the anchor to have <code>ID = 4</code>. Then in the second part we join the <code>MANAGER_ID</code> of the base table with the <code>ID</code> of the CTE (the previous record). So what it says is give me the  person whose manager was in the previous loop.</p> <p></p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/CTE_recursion/#example-going-up-the-tree","title":"Example going up the tree","text":"<p>In this example we are still Donnie. Now we want to know who his manager is, and their manager etc. We do exactly the same as the previous example, but now we need to revert the join condition.</p> <pre><code>WITH CTE_Recursive AS\n(\n    SELECT H.*\n    FROM #Hierarchy H\n    WHERE ID = 4\n\n    UNION ALL\n\n    SELECT H.*\n    FROM #Hierarchy AS H\n        JOIN CTE_Recursive AS R\n            ON H.ID = R.MANAGER_ID\n            AND H.ID &lt;&gt; H.MANAGER_ID\n)\nSELECT *\nFROM CTE_Recursive\n</code></pre> <p>Now the <code>MANAGER_ID</code> is taken from the CTE (the previous loop). However, if we look more closely, we  that there is an infinite loop. Alice is manager of herself. To break out of this loop we need to  say that for the new record we want to add the <code>MANAGER_ID</code> can not be the same as the <code>ID</code>. We get  the following result.</p> <p></p> <p>As you can see there is no Alice in the results. This is because the record of Alice does not satisfy the join condition and thus will not be added. What we need is another condition that loosens up the previous condition. We will allow the <code>MANAGER_ID</code> to be the same as the <code>ID</code> as long as it just occurs once. </p> <pre><code>WITH CTE_Recursive AS\n(\n    SELECT H.*\n    FROM #Hierarchy H\n    WHERE ID = 4\n\n    UNION ALL\n\n    SELECT H.*\n    FROM #Hierarchy AS H\n        JOIN CTE_Recursive AS R\n            ON H.ID = R.MANAGER_ID\n            AND (H.ID &lt;&gt; H.MANAGER_ID OR R.ID &lt;&gt; H.ID)\n\n)\nSELECT *\nFROM CTE_Recursive\n</code></pre> <p>Here we have added the condition that we can let the record pass when the <code>ID</code> of the employee is  different than the <code>ID</code> of the employee in the previous recursive loop. Now Alice shows up once  correctly.</p> <p></p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/CTE_recursion/#technical-explanation","title":"Technical explanation","text":"<p>Warning</p> <p>CTE recursion is not real recursion, it is just iteration.</p> <p>From the Postgres docs: </p> <p>Quote</p> <p>Note: Strictly speaking, this process is iteration not recursion,  but <code>RECURSIVE</code> is the terminology chosen by the SQL standards committee.</p> <p>From the archives of Microsoft docs it can also be concluded that only the results of the previous run are available.</p> <p>Quote</p> <p>Once it finishes executing the anchor part of the plan,  the query processor continues by executing the second input to the concatenation operator  which happens to be the recursive part of the plan.  This part of the plan has a nested  loops join with the secondary spool as its outer input.  The nested loops join requests a  row from the spool which so far contains only the one row for employee 109. The spool returns this row and deletes it from the worktable.  The join then executes the  index seek on its inner input to find all employees who work for employee 109. This index seek returns the following rows which the primary spool inserts into the worktable:</p> <p>Also this post on stackexchange concludes that it is not real recursion.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/helpful_resources/","title":"Useful resources and links","text":""},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/helpful_resources/#websitesblogs","title":"Websites/blogs","text":"<ul> <li>https://www.brentozar.com/</li> <li>https://www.cathrinewilhelmsen.net/</li> <li>https://ola.hallengren.com/</li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/helpful_resources/#videos","title":"Videos","text":"<ul> <li>How to Think Like the SQL Server Engine Part 1</li> <li>How to Think Like the SQL Server Engine Part 2</li> <li>How to Think Like the SQL Server Engine Part 3</li> <li>How to Think Like the SQL Server Engine Part 4</li> <li>How to Think Like the SQL Server Engine Part 5</li> <li>How to Think Like the SQL Server Engine Part 6</li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/","title":"Indexes","text":"<p>Indexes can be very useful to speedup queries. However, overdoing them could cause the opposite. This page describes what indexes are, which types of indexes are present and when to use which. But first of all, we need to understand how data is stored in SQL server.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#how-data-is-stored","title":"How data is stored","text":"<p>Data in SQL server is stored in pages, pages that are 8 KB big. A row is stored in such a page and cannot span multiple pages. What can be done is that if some columns are too large to make the row fit, instead of saving the contents directly on the same page, a pointer will be saved. This pointer will refer to other pages that contain the  data.</p> <p>For more information see: https://docs.microsoft.com/en-us/sql/relational-databases/pages-and-extents-architecture-guide</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#indexes-in-a-nutshell","title":"Indexes in a nutshell","text":"<p>In essence an index is nothing more than an ordered list. By knowing which columns are sorted it makes looking up the correct records much faster as you \"know\" where that record is.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#table-example","title":"Table example","text":"<p>Let's look at following simple example of an unordered list:</p> fruit_id fruit_name 7 Lychee 2 Blueberry 4 Orange 3 Banana 8 Pineapple 5 Mango 1 Apple 9 Strawberry 10 Melon 6 Avocado <p>If you want to get the fruit with <code>id = 1</code>, then you can't just pick the first one, because it is unsorted.  You would need go through the list until you find the correct record. This is a small table and thus the record can be found relatively easily, however, it still takes more time than when you know you can just pick the first one.</p> <p>So in case the <code>id</code> is used often in queries, it would make it simpler to have the records sorted on that column.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#book-example","title":"Book example","text":"<p>Let's also take a book as analogy. Consider each page to be a row, a row consisting of 3 columns:</p> <ul> <li>The page number</li> <li>The chapter name/number</li> <li>The contents</li> </ul> <p>In the case of a book the pages are sorted ordered by the page number. If you look at the chapter numbers you will see that they are also sorted. This makes it very easy to jump to a certain page or chapter.</p> <p>In the next sections we will expand on this book example to explain the concepts of indexes.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#index-seeks-and-index-scans","title":"Index seeks and index scans","text":"<p>As seen from the examples in the previous section, when there is an index (when the list is ordered) you can relatively quickly get to the correct row, in case the column you are looking in is part of the index. This  action of going through an index is called index seek.</p> <p>When there is no index present, you would have to go row by row and page by page to ge to the correct row. This  takes considerably longer. This action of going through the rows is called index scan. Most of the time you want to avoid index scans.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#types-of-indexes","title":"Types of indexes","text":"<p>So now we have a rough idea what indexes are and why they are handy, we can have a look what types of indexes there are. We can distinguish the following index types:</p> <ul> <li>Heap (no index)</li> <li>Row-store indexes<ul> <li>Clustered</li> <li>Non-clustered</li> </ul> </li> <li>Column-store indexes  <ul> <li>Clustered</li> <li>Non-clustered</li> </ul> </li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#the-heap-no-index","title":"The heap (no index)","text":"<p>A table that has index type heap, means that it does not have an index. It just piles the rows in the order it  receives. Having no index means that getting to a row might be slower. However, because the row is just written at the end, there is no need to put the row between existing rows and existing indexes do not need to be rewritten. This means that writing will be a lot faster.</p> <p>When inserted lots of rows, it might be beneficial to remove the index, then insert the rows and finally redo the indexes. For staging tables you should opt for this strategy.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#clustered-vs-non-clustered","title":"Clustered vs non-clustered","text":"<p>A clustered index is sorting the data itself, while a non-clustered index is keeping a separate ordered list. Clustered and non-clustered indexes can be easily explained by revisiting the book example. The pages are physically ordered, the text from those pages also move around when ordering the pages by page number. It IS your data, so when building a clustered index, it takes longer, as all the data needs to be moved with it.</p> <p>An example of a non-clustered index is the table of contents, it is a separate (much shorter) list that points to the page number where that chapter starts.Non-clustered indexes always will point to the clustered index. So you look  up the page number and then using that page number you seek through the clustered index to get to your page.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#includes","title":"Includes","text":"<p>As discussed earlier, the clustered index IS your entire row and thus consists of all the columns. This means that if you get to the row, you will have access to all data. With a non-clustered index, you only have the columns that were part of the index (that were used to sort). This means that you need to make an extra step to get to that data. However, there is a way to include extra columns in the clustered index.</p> <p>Example: <pre><code>CREATE NONCLUSTERED INDEX IX_Address_PostalCode  \nON Person.Address (PostalCode)  \nINCLUDE (AddressLine1, AddressLine2, City, StateProvinceID);  \n</code></pre></p> <p>This should only be used for smaller columns and when that column is really needed often, otherwise avoid includes as it will increase the size of your storage.</p> <p>For more information: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/create-indexes-with-included-columns</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/indexes/#row-vs-column-store","title":"Row vs column store","text":"<p>As the names imply row-store stores the rows row by row and a column-store stores the data in a column-wise fashion. The advantage of a column-store is that it can be compressed very well. The reason that compression can be done much better is that contents belonging to the same row look more alike. They have the same datatype, follow the  same pattern and/or are part of en enumeration (which limits the set of possibilities). </p> <p>So let's say we have a column that contains the status (for example: started, closed, in progress etc.). There is  a finite number of statuses. So if we sort on the status, then it might be the case that on 1 8KB page there is only 1 status present. In that case you could just say that all records on this page have status \"started\" for example, which will sae a lot in terms of storage. </p> <p>However, there are also drawback. The data will only be columnar compressed when it reaches a threshold of 1,048,576 rows. Everything else will be stored in a delta row group.</p> <p>For more information see: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver15</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/recommended_server_settings/","title":"Recommended server settings","text":"<p>When you first install SQL server, it comes with some defaults that might not be optimal. In this page we are going over some of them and what the recommended setting should be. This is based on experience, but also backed up with some online resources.</p> <p>Consulted resources:</p> <ul> <li>https://www.brentozar.com/archive/2013/09/five-sql-server-settings-to-change/</li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/recommended_server_settings/#maximum-degree-of-parallelism","title":"Maximum Degree of Parallelism","text":"<p>This setting indicates how many parallel tasks the sql server engine uses during a query execution. By having this setting too high will cause some queries to occupy all available resources. To prevent this you should limit this number. </p> <p>Brent Ozar recommends to set it to the amount of physical cores available. However, this is not desired when the server is used by lots of users and processes at the same time. So play around and see what works best.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/recommended_server_settings/#cost-threshold-for-parallelism","title":"Cost Threshold for Parallelism","text":"<p>The default value is set to 5, which is too low. This results in the query engine to split the execution in too many threads. Performance wise this is not desired. </p> <p>Brent Ozar recommends setting this to 50. From experience this works well.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/recommended_server_settings/#number-of-files-for-tempdb","title":"Number of files for tempdb","text":"<p>by having just 1 file, all processor cores would want access to that file and this would slow things down. As nowadays compute is not the limiting factor, rather the IO. However, having too many causes also synchronisation slow downs.</p> <p>Refer to the following page for more info: http://www.sqlskills.com/blogs/paul/a-sql-server-dba-myth-a-day-1230-tempdb-should-always-have-one-data-file-per-processor-core/</p> <p>Which has been summarised by a user on stackexchange by: According to Paul Randal the number of tempdb files should be:</p> <ul> <li>equal to the number of CPU cores for 8 or less cores</li> <li>1/4 to 1/2 of CPU cores for more than 8 cores</li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/recommended_server_settings/#compression","title":"Compression","text":"<p>Enable compression on all tables. By having smaller tables queries will run much faster. The IO is often the  limiting factor. <code>PAGE COMPRESSION</code> would be a good point to start.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/GenerateTemplatedQueries/","title":"Generate Queries using templates","text":"<p>Sometimes it is handy to be able to generate queries that adhere to the same format. The most simple example would be to generate a bunch of queries that does a <code>SELECT *</code> from a table. The list of tables can be provided manually or taken from the information schema. The only thing that needs to be done is to fill in the table names.</p> <p>What some people tend to do is to use <code>+</code> to concatenate the parts of the query to be generated with the  variable table name, table schema etc. This however, will quickly become an unreadable mess. A better way to do  this is by making use of query templates. Query templates are just strings (<code>VARCHAR</code>) that have placeholders in them. The only thing you then need to do is replace those when running the query. </p> <p>Choice of placeholder format</p> <p>It is important to choose a distinct placeholder to avoid potential collisions. Recommended is to put curly brackets <code>{}</code> around the name of the placeholder. In SQL curly brackets are not used.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/GenerateTemplatedQueries/#simple-example","title":"Simple example","text":"<p>Here we give an example of how the query looks like with and without the placeholder strategy. The example is the <code>SELECT * FROM</code>.</p> PlaceholderUsing concatenation <pre><code>DECLARE @template VARCHAR(MAX) =  \n' \nSELECT TOP 10 * \nFROM {schema}.{table} \n' \n\nSELECT  \n    [Database]  = T.TABLE_CATALOG \n,   [Schema]    = T.TABLE_SCHEMA \n,   [Table]     = T.TABLE_NAME \n,   [Query]     =   REPLACE( \n                    REPLACE( @template, '{schema}', T.TABLE_SCHEMA) \n                                        , '{table}', T.TABLE_NAME) \nFROM INFORMATION_SCHEMA.TABLES AS T \n</code></pre> <pre><code>SELECT  \n    [Database]  = T.TABLE_CATALOG \n,   [Schema]    = T.TABLE_SCHEMA \n,   [Table]     = T.TABLE_NAME \n,   [Query]     = 'SELECT TOP 10 * FROM '+T.TABLE_SCHEMA+'.'+T.TABLE_NAME \nFROM INFORMATION_SCHEMA.TABLES AS T\n</code></pre> <p>For this simple example, the concatenation method is shorter and maybe just as readable. So let's look at a more complex example.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/GenerateTemplatedQueries/#more-complex-example-generate-create-table-statement","title":"More complex example: Generate CREATE TABLE statement","text":"<p>In general when you want to create a table from an existing table, you could do that by right-clicking  on the table and choosing <code>Script Table as -&gt; CREATE To</code>. However, when you want to get a create table statement  on the fly and running it afterwards in a stored procedure, you are not able to use that feature.</p> <p>So this example focuses on the generation of the CREATE TABLE statement. In this example we keep it as simple as possible by only considering the column names and datatypes. </p> Placeholder <pre><code>DECLARE @template VARCHAR(MAX) = \n'\nCREATE TABLE [{schema}].[{table}](\n{column_definitions}\n)\n'\n\nDECLARE @column_definition_template VARCHAR(MAX) =\n',  [{column_name}] [{column_datatype}]\n'\n\n\nSELECT \n    [Database]  =   T.TABLE_CATALOG\n,   [Schema]    =   T.TABLE_SCHEMA\n,   [Table]     =   T.TABLE_NAME\n,   [Query]     =   REPLACE(\n                    REPLACE(\n                    REPLACE(    @template,  '{schema}', T.TABLE_SCHEMA)\n                                        ,   '{table}', T.TABLE_NAME)\n                                        ,   '{column_definitions}', CD.Column_definition)\nFROM INFORMATION_SCHEMA.TABLES AS T\n    /* \n        This cross apply is to get the aggregated string of all the \n        column definitions.\n    */\n    CROSS APPLY(\n        /* \n            Remove the leading comma from the template \n        */\n        SELECT STUFF(A.Column_definition.value('.','NVARCHAR(MAX)'), 1,1,'')\n        FROM\n        (\n            /*\n             This most inner SELECT is to fill in the colmn definition template.\n             The template will be filled in for each column\n             */\n            SELECT \n\n                [Column_definition]     =   REPLACE(\n                                            REPLACE(    @column_definition_template, '{column_name}', C.COLUMN_NAME)\n                                                                                    ,'{column_datatype}', C.DATA_TYPE)\n            FROM INFORMATION_SCHEMA.COLUMNS AS C\n            WHERE 1=1\n                AND C.TABLE_CATALOG = T.TABLE_CATALOG\n                AND C.TABLE_SCHEMA = T.TABLE_SCHEMA\n                AND C.TABLE_NAME = T.TABLE_NAME\n            ORDER BY C.ORDINAL_POSITION\n            FOR XML PATH, TYPE\n        ) AS A(Column_definition)\n    ) AS CD(Column_definition)\nORDER BY [Database],[Schema],[Table]\n</code></pre> <p>Although the query as a whole might look complex, by having query templates it allows the reader of the code to have a better overview where the query is aiming for. You can immediately see that there are 2 simple templates involved:</p> <ul> <li>One containing the overall structure of the CREATE TABLE</li> <li>One containing the building block for a column with datatype</li> </ul>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/GroupMembersQuery/","title":"Query users in groups","text":"<p>It is recommended to have the authorisation on AD-group level, rather than on user level. However, this means that the <code>Logins</code> on your SQL server will only have the AD-groups. In case you want to know who is in those groups, you can make use of the extended procedure  xp_logininfo.</p> <p>By calling the procedure without any arguments, you will get a list of all logins and also what type of  login it is. For example:</p> <ul> <li>group</li> <li>user</li> </ul> <pre><code>EXEC xp_logininfo\n</code></pre> <p>Once you have found the group you want to have more details about the members, you can run the  following query.</p> <pre><code>EXEC xp_logininfo '&lt;your-chosen-group&gt;', 'members'\n</code></pre>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/GroupMembersQuery/#expand-all-groups","title":"Expand all groups","text":"<p>In case you want to expand all groups and do not want to go over each group manually, you can use the following query:</p> <pre><code>IF OBJECT_ID('tempdb..#AllRoleMappings') IS NOT NULL\n    DROP TABLE #AllRoleMappings\n\nDECLARE @AllLogins TABLE\n(\n    AccountName         varchar(200)\n,   type                varchar(10)\n,   privilege           varchar(10)\n,   mappedLoginName     varchar(100)\n,   permissionPath      varchar(1000)\n)\n\nCREATE TABLE #AllRoleMappings \n(\n    AccountName         varchar(200)\n,   type                varchar(10)\n,   privilege           varchar(10)\n,   mappedLoginName     varchar(100)\n,   permissionPath      varchar(1000)\n)\n\nINSERT INTO @AllLogins\nEXEC xp_logininfo\n\nDECLARE @CurrentGroup varchar(200);\n\nDECLARE CUR_Groups CURSOR  \n    FOR SELECT AccountName FROM @AllLogins WHERE Type IN ('group')\n\nOPEN CUR_Groups\nFETCH NEXT FROM CUR_Groups   \nINTO @CurrentGroup\n\nWHILE @@FETCH_STATUS = 0  \nBEGIN  \n      INSERT INTO #AllRoleMappings\n      EXEC xp_logininfo @CurrentGroup, 'members'\n\n      FETCH NEXT FROM CUR_Groups INTO @CurrentGroup \nEND \n\nCLOSE CUR_Groups  \nDEALLOCATE CUR_Groups \n\nSELECT *\nFROM #AllRoleMappings\n</code></pre>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/QuerySSISPackages/","title":"Querying your SSIS packages","text":"<p>For analysis purposes it might be handy to look for a certain string in the package definition. For SSIS packages  in the package store, you can use the following query.</p> Package Store <pre><code>SELECT TOP 1000\n    F.foldername\n,   P.[name]\n,   P.[id]\n,   P.[description]\n,   [XML_content]       = CAST(CONVERT(VARCHAR(MAX),CONVERT(VARBINARY(MAX), packagedata)) AS XML)\n,   P.[createdate]\n,   P.[folderid]\n,   P.[ownersid]\n,   P.[packagedata]\n,   P.[packageformat]\n,   P.[packagetype]\n,   P.[vermajor]\n,   P.[verminor]\n,   P.[verbuild]\n,   P.[vercomments]\n,   P.[verid]\n,   P.[isencrypted]\n,   P.[readrolesid]\n,   P.[writerolesid]\nFROM [msdb].[dbo].[sysssispackages] P\n    JOIN msdb.dbo.sysssispackagefolders F\n        ON P.folderid = F.folderid\nWHERE 1=1\n    AND CONVERT(VARCHAR(MAX),CONVERT(VARBINARY(MAX), packagedata)) LIKE '%&lt;you search string&gt;%'\n</code></pre>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/useful_scripts/SQLdependencies/","title":"SQL dependencies","text":"<p>In SSMS you can right click a database object such as a table or view to view the dependencies. These dependencies  are not bound to physical relations such as foreign keys. If the object is referenced in another view or store procedure, it will also show up.</p> <p>Manually requesting the dependencies might not be the best way to go. So I have written a script to find all the  usages across all the databases.</p> <pre><code>/*\n *  Clean up #TEMP if already exists\n */\nIF OBJECT_ID('tempdb..#TEMP') IS NOT NULL\n    DROP TABLE #TEMP\n\nDECLARE @ReferencedDatabase     NVARCHAR(50)        = N'&lt;Enter database name of referenced entity&gt;';\n/* Schemaname: Leave empty to show all */\nDECLARE @ReferencedSchemaName   NVARCHAR(100)       = N'%dbo%'; \n/* Entiteitname (Tables/views etc): Leave empty to show all */\nDECLARE @ReferencedEntityName   NVARCHAR(100)       = N'%%';    \n\nDECLARE @sql NVARCHAR(2000) =\n'\nUSE ?;\n\nINSERT INTO ##Temp\n\nSELECT \n       [Referencing Database]            =      ''?'' \n,      [Referencing Schema]       =      s.name \n,      [Referencing Object name]  =      ao.name \n,      [Referencing Object Type]  =      ao.type_desc \n,      [Referenced Database]             =      sql.referenced_database_name \n,      [Referenced Schema]               =      sql.referenced_schema_name \n,      [Referenced Object name]   =      sql.referenced_entity_name \n,      [Referenced Object Type]   =      aoReferenced.type_desc \n,      [ReferencedStillExists]            =      CASE WHEN aoReferenced.name IS NULL THEN 0 ELSE 1 END \nFROM sys.sql_expression_dependencies sql \n       JOIN sys.all_objects ao \n             ON ao.object_id = sql.referencing_id \n       JOIN sys.schemas s \n             ON s.schema_id = ao.schema_id \n       LEFT JOIN \n       ( \n             {ReferencedDatabase}.sys.all_objects aoReferenced \n             JOIN   {ReferencedDatabase}.sys.schemas aoReferencedSchema \n             ON aoReferenced.schema_id = aoReferencedSchema.schema_id     \n       ) \n             ON aoReferenced.name COLLATE Latin1_General_CI_AS = sql.referenced_entity_name COLLATE Latin1_General_CI_AS\n             AND aoReferencedSchema.name COLLATE Latin1_General_CI_AS = sql.referenced_schema_name COLLATE Latin1_General_CI_AS\nWHERE 1=1\n    AND (referenced_database_name = ''{ReferencedDatabase}'' OR (referenced_database_name IS NULL AND ''{ReferencedDatabase}'' = ''?''))\n    AND (referenced_schema_name LIKE ''{ReferencedSchema}'' OR ''{ReferencedSchema}'' = '''')\n    AND (referenced_entity_name LIKE ''{ReferencedEntity}'' OR ''{ReferencedEntity}'' = '''')\nORDER BY [Referencing Database], [Referencing Schema], [Referencing Object name]\n'\n\nIF OBJECT_ID('tempdb..##Temp') IS NOT NULL\n       DROP TABLE ##Temp\n\nCREATE TABLE ##Temp \n(\n       [Referencing Database]           NVARCHAR(200)\n,      [Referencing Schema]             NVARCHAR(200)\n,      [Referencing Object name]        NVARCHAR(200)\n,      [Referencing Object Type]        NVARCHAR(200)\n,      [Referenced Database]            NVARCHAR(200)\n,      [Referenced Schema]              NVARCHAR(200)\n,      [Referenced Object name]         NVARCHAR(200)\n,      [Referenced Object Type]         NVARCHAR(200)\n,      [ReferencedStillExits]           NVARCHAR(200)\n);\n\nSET @sql    =   REPLACE(\n                REPLACE(    \n                REPLACE(    @sql,       '{ReferencedDatabase}',     @ReferencedDatabase)\n                                ,       '{ReferencedSchema}',       @ReferencedSchemaName)\n                                ,       '{ReferencedEntity}',       @ReferencedEntityName)\n\nexec sp_MSforeachdb @sql\n\n/* Switch to local temp table */\nSELECT DISTINCT *\nINTO #TEMP\nFROM ##Temp\n\nDROP TABLE ##TEMP\n\n/* Show results */\nSELECT *\nFROM #TEMP\n</code></pre> <p>In this query notice the following line: <pre><code>AND (referenced_database_name = ''{ReferencedDatabase}'' OR (referenced_database_name IS NULL AND ''{ReferencedDatabase}'' = ''?''))\n</code></pre> It might be possible that in a view the database name is not mentioned. Therefore the dependencies view will show <code>NULL</code>. However, we know that when we don't have a database name, it means it will look for it in the current database. So  that is why there is that check: <code>(referenced_database_name IS NULL AND ''{ReferencedDatabase}'' = ''?''))</code>.</p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/workaround_older_versions/string_aggregate/","title":"String aggregate","text":"<p>In SQL server versions 2017 and later, there exist the function STRING_AGG to combine the contents of multiple rows with a delimiter. </p>"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/workaround_older_versions/string_aggregate/#the-workaround","title":"The workaround","text":"<p>The trick is to create XML from your query result. The XML output needs to be configured in such a  way that it does not contain tags, or at least contain just 1 tag which has no nesting.  By doing so the contents of all the rows will be concatenated.</p> <p>Let's walk through it step by step. Beginning with creating a simple table to perform the query on. And let's say we want to aggregate the table based on the <code>CATEGORY</code> while concatenating the <code>FRUIT</code> names with a comma.</p> <pre><code>DECLARE @table TABLE(\n    ID INT IDENTITY\n,   CATEGORY INT\n,   FRUIT VARCHAR(50)\n)\n\nINSERT INTO @table\n(CATEGORY, FRUIT)\nVALUES\n(1,     'Apple'),\n(1,     'Banana'),\n(1,     'Orange'),\n(2,     'Watermelon'),\n(2,     'Pineapple')\n</code></pre> <p>If we query this table and simply apply the <code>FOR XML PATH</code></p> <p><pre><code>SELECT FRUIT\nFROM @table\nFOR XML PATH\n</code></pre> we get:</p> <pre><code>&lt;row&gt;\n  &lt;FRUIT&gt;Apple&lt;/FRUIT&gt;\n&lt;/row&gt;\n&lt;row&gt;\n  &lt;FRUIT&gt;Banana&lt;/FRUIT&gt;\n&lt;/row&gt;\n&lt;row&gt;\n  &lt;FRUIT&gt;Orange&lt;/FRUIT&gt;\n&lt;/row&gt;\n&lt;row&gt;\n  &lt;FRUIT&gt;Watermelon&lt;/FRUIT&gt;\n&lt;/row&gt;\n&lt;row&gt;\n  &lt;FRUIT&gt;Pineapple&lt;/FRUIT&gt;\n&lt;/row&gt;\n</code></pre> <p>To remove the layer with the <code>&lt;row&gt;</code> tags we pass an empty string to the <code>PATH</code>. We also add the <code>TYPE</code> directive. If you do not convert it to XML type using the <code>TYPE</code> directive, then special characters will be  HTML encoded. See this post on stackoverflow: https://stackoverflow.com/questions/15643683/how-do-i-avoid-character-encoding-when-using-for-xml-path. The greater than sign <code>&gt;</code> for example will then become <code>&amp;lt;</code>.</p> <pre><code>SELECT FRUIT\nFROM @table\nFOR XML PATH(''), TYPE\n</code></pre> <p>Then we have: <pre><code>&lt;FRUIT&gt;Apple&lt;/FRUIT&gt;\n&lt;FRUIT&gt;Banana&lt;/FRUIT&gt;\n&lt;FRUIT&gt;Orange&lt;/FRUIT&gt;\n&lt;FRUIT&gt;Watermelon&lt;/FRUIT&gt;\n&lt;FRUIT&gt;Pineapple&lt;/FRUIT&gt;\n</code></pre></p> <p>Adding the <code>TYPE</code> directive in our example has no visible effect.</p> <p>In order to turn the generated XML into a string we use the <code>value</code> function. That takes as inputs the path to the nodes we want to get the value from and the data type of that node.</p> <p>As we have no nested structure we can just pass <code>'.'</code> as the path, which just means the root node. As datatype we pass <code>nvarchar(max)</code> just to be sure it fits. You can of course put something smaller.</p> <p><pre><code>SELECT(\n    SELECT ', '+FRUIT\n    FROM @table\n    FOR XML PATH(''), TYPE\n).value('.','nvarchar(max)')\n</code></pre> Resulting in  <pre><code>, Apple, Banana, Orange, Watermelon, Pineapple\n</code></pre></p> <p>We have put the delimiter in the front, because that makes it easier to remove. If we would have put it at  the end, we would need to know how long the string is. To remove characters at a certain location we use the STUFF function.</p> <p>Finally we need to write the outer query that contains the list of categories. We use  an <code>APPLY</code> to concatenate the string for each category.</p> <pre><code>SELECT \n    CATEGORY\n,   RES.AGG\nFROM @table AS A\nOUTER APPLY\n(\n    SELECT  \n        [AGG] = STUFF(\n                        (\n                            SELECT ', '+FRUIT\n                            FROM @table AS B\n                            WHERE 1=1\n                                AND A.CATEGORY = B.CATEGORY\n                            FOR XML PATH(''), TYPE\n                        ).value('.','nvarchar(max)')\n                ,1,2,'')\n) AS RES\nGROUP BY\n    CATEGORY\n,   RES.AGG\n</code></pre> CATEGORY AGG 1 Apple, Banana, Orange 2 Watermelon, Pineapple"},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/workaround_older_versions/string_split/","title":"String split","text":"<p>In SQL server versions 2016 and later, there exist the function STRING_SPLIT to split the contents of a column based on a delimiter. Each split element will become a new row.</p> <p><pre><code>SELECT value FROM STRING_SPLIT('Lorem ipsum dolor sit amet.', ' ');\n</code></pre> Has the following table as output:</p> value Lorem ipsum dolor sit amet."},{"location":"Data_Engineering_Analysis/Microsoft/SQL_server/workaround_older_versions/string_split/#the-workaround","title":"The workaround","text":"<p>The trick is by wrapping the contents in XML tags and turn each element into an XML node.</p> <ol> <li>Prepend the string to be split with an opening tag <code>&lt;X&gt;</code></li> <li>Replace the delimeter with a closing tag followed directly with an opening tag <code>&lt;/X&gt;&lt;X&gt;</code></li> <li>At the end of the string put the last closing tag <code>&lt;/X&gt;</code></li> </ol> <p>So if you have a string that looks like  <pre><code>A,B,C,D,E\n</code></pre> then after the modifications you will have <pre><code>&lt;X&gt;A&lt;/X&gt;&lt;X&gt;B&lt;/X&gt;&lt;X&gt;C&lt;/X&gt;&lt;X&gt;D&lt;/X&gt;&lt;X&gt;E&lt;/X&gt;\n</code></pre></p> <p>After these preparations are done, you can use SQL XML functions combined with a <code>CROSS APPLY</code> to give each node a separate row.</p> <p>An example from mssqltips: https://www.mssqltips.com/sqlservertip/1771/splitting-delimited-strings-using-xml-in-sql-server/</p> <pre><code>DECLARE @t TABLE( ID INT IDENTITY, data VARCHAR(50))\nINSERT INTO @t(data) SELECT 'AA,AB,AC,AD'\nINSERT INTO @t(data) SELECT 'BA,BB,BC'\n\nSELECT \n    F1.id,\n    F1.data,\n    O.splitdata \nFROM\n(\n    SELECT \n        *\n    ,   CAST('&lt;X&gt;'+replace(F.data,',','&lt;/X&gt;&lt;X&gt;')+'&lt;/X&gt;' as XML) as xmlfilter \n    FROM @t F\n)F1\nCROSS APPLY\n( \n     SELECT fdata.D.value('.','varchar(50)') as splitdata \n     FROM f1.xmlfilter.nodes('X') as fdata(D)\n) O\n</code></pre> <p></p>"},{"location":"Data_Engineering_Analysis/airflow/scheduling/","title":"Scheduling","text":""},{"location":"Data_Engineering_Analysis/airflow/scheduling/#format","title":"Format","text":"<p>The format in which the schedule is defined is using crontab. It consists of 5 positions, going from left to right:</p> <ol> <li>minute</li> <li>hour</li> <li>day (of month)</li> <li>month</li> <li>day (of week)</li> </ol> <p>At each position you can use:</p> <ul> <li><code>*</code> indicating all values. If for exampel an <code>*</code> is in the minute position, it means that the schedule runs every minute.</li> <li><code>,</code> with the comma you can give a list of numbers.</li> <li><code>-</code> with the dash you can give a range of numbers.</li> <li><code>/</code> indicates the step size it takes. <code>/4</code> in the minutes position means that every 4th minute the schedule is going to run.</li> </ul> <p>The allowed numerical values for each of the positions are:</p> Cron tab position Allowed range of numerical values Minute 0-59 Hour 0-23 Day (of month) 1-31 Month 1-12 Day (of week) 0-6, where 0 equals Sunday <p>To play around with this notation, go to https://crontab.guru/.</p>"},{"location":"Data_Engineering_Analysis/airflow/scheduling/#when-does-airflow-actually-start-the-schedule","title":"When does airflow actually start the schedule","text":"<p>If you have the schedule <code>0 * * * *</code>, it will run every hour at minute <code>:00</code>. Let's say that the current time is <code>19:48</code>, you would expect that at <code>20:00</code> the schedule for <code>20:00</code> will be run. This, however, is not the case. At <code>20:00</code> the schedule for <code>19:00</code> will be run. Airflow runs the schedule at the end of the scheduling period.</p> <p>You can read more about it here: https://towardsdatascience.com/apache-airflow-tips-and-best-practices-ff64ce92ef8</p>"},{"location":"Data_Engineering_Analysis/general/detect-overlapping-timelines/","title":"Overlapping timelines","text":"<p>Let's say we have a source that keeps track of changes, and we ingest the source  with all the changes. Each change has a validity range. On this page we will use the columns <code>JOURNALNO_ID</code> and <code>JOURNALENDNO_ID</code> to define this validity range. To keep things simple, these columns will be integers. To indicate that a record is valid, the <code>JOURNALENDNO_ID</code> will be set to the integer maximum (2147483647).</p> <p>The convention that is used is that the number in the <code>JOURNALENDNO_ID</code> is not included. This means that we have the same number in the <code>JOURNALENDNO_ID</code> as the <code>JOURNALNO_ID</code>  of the next row, if there is no gap.</p> <p>Let's consider the following example:</p> PK_ID JOURNALNO_ID JOURNALENDNO_ID A 1 3 A 3 8 A 4 9 A 9 20 B 3 2147483647 B 7 2147483647 <p>In this example, we have 2 overlapping records for the primary key <code>A</code> and 2 overlapping records for primary key <code>B</code>.</p>"},{"location":"Data_Engineering_Analysis/general/detect-overlapping-timelines/#logic-of-the-checks","title":"Logic of the checks","text":"<p>In order to find all the overlapping records, we need to perform 2 window function checks.</p>"},{"location":"Data_Engineering_Analysis/general/detect-overlapping-timelines/#the-maximum-journalendno_id-of-the-previous-rows","title":"The maximum JOURNALENDNO_ID of the previous rows","text":"<p>The <code>JOURNALENDNO_ID</code> of a previous row should ALWAYS be smaller or equal to the <code>JOURNALNO_ID</code> of the current row. If that is not the case then the previous record is still active when the current row is active.</p> <pre><code>WITH cte_data AS\n(\n  SELECT col1 AS PK_ID, col2 AS JOURNALNO_ID, col3 AS JOURNALENDNO_ID\n  FROM\n      VALUES\n          ('A',1,3)\n      ,   ('A',3,8)\n      ,   ('A',4,9)\n      ,   ('A',9,20)\n      ,   ('B',3,2147483647)\n      ,   ('B',7,2147483647)\n)\nSELECT *\n-- The maximum JOURNALENDNO_ID of te previous rows\n, MAX(cte.JOURNALENDNO_ID) OVER (PARTITION BY cte.PK_ID ORDER BY cte.JOURNALNO_ID ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS max_previous_end\nFROM cte_data as cte\n</code></pre> <p></p> <p>As can be seen, only 1 of the overlapping records were found. The next check will find the other one.</p>"},{"location":"Data_Engineering_Analysis/general/detect-overlapping-timelines/#the-minimum-journalno_id-of-the-following-rows","title":"The minimum JOURNALNO_ID of the following rows","text":"<p>The <code>JOURNALENDNO_ID</code> of the current row should ALWAYS be smaller or equal to the <code>JOURNALNO_ID</code> of the following  rows. Otherwise, the current row is not properly closed while the next row is starting.</p> <pre><code>WITH cte_data AS\n(\n  SELECT col1 AS PK_ID, col2 AS JOURNALNO_ID, col3 AS JOURNALENDNO_ID\n  FROM\n      VALUES\n          ('A',1,3)\n      ,   ('A',3,8)\n      ,   ('A',4,9)\n      ,   ('A',9,20)\n      ,   ('B',3,2147483647)\n      ,   ('B',7,2147483647)\n)\nSELECT *\n-- The minimum JOURNALNO_ID of the following rows\n, MIN(cte.JOURNALNO_ID) OVER (PARTITION BY cte.PK_ID ORDER BY cte.JOURNALNO_ID ROWS BETWEEN 1 FOLLOWING AND UNBOUNDED FOLLOWING) AS min_following_begin\nFROM cte_data as cte\n</code></pre> <p></p>"},{"location":"Data_Engineering_Analysis/general/detect-overlapping-timelines/#the-full-check","title":"The full check","text":"<p>In this example, we assume that your SQL supports <code>QUALIFY</code>. If <code>QUALIFY</code> is not supported, you need to create an additional CTE for filtering.</p> <pre><code>WITH cte_data AS\n(\n  SELECT col1 AS PK_ID, col2 AS JOURNALNO_ID, col3 AS JOURNALENDNO_ID\n  FROM\n      VALUES\n          ('A',1,3)\n      ,   ('A',3,8)\n      ,   ('A',4,9)\n      ,   ('A',9,20)\n      ,   ('B',3,2147483647)\n      ,   ('B',7,2147483647)\n)\nSELECT *\n-- The maximum JOURNALENDNO_ID of te previous rows\n, MAX(cte.JOURNALENDNO_ID) OVER (PARTITION BY cte.PK_ID ORDER BY cte.JOURNALNO_ID ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS max_previous_end\n\n-- The minimum JOURNALNO_ID of the following rows\n, MIN(cte.JOURNALNO_ID) OVER (PARTITION BY cte.PK_ID ORDER BY cte.JOURNALNO_ID ROWS BETWEEN 1 FOLLOWING AND UNBOUNDED FOLLOWING) AS min_following_begin\nFROM cte_data as cte\nQUALIFY\n-- Overlap when the JOURNALNO_ID of the current row is smaller\n   than the maximum JOURNALENDNO_ID of te previous rows\n  cte.JOURNALNO_ID &lt; max_previous_end\n\n-- Overlap when the JOURNALENDNO_ID of the current row is larger\n   than the minimum JOURNALNO_ID of the following rows\nOR cte.JOURNALENDNO_ID &gt; min_following_begin\n</code></pre> <p></p>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/","title":"Merging timelines","text":"<p>Let's say we have the 2 timelines as shown in the picture below. For simplicity sake we  mark the nodes on the timeline with the numbers 0 to 10. As can be seen, the ranges do not always coincide.  </p> <p>In practice this could be 2 satellites in a datavault model connecting to the same hub. In your datamart you might want to have all attributes put together in chronological order. </p>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#concept","title":"Concept","text":"<p>The first step is to create a third timeline that has all the nodes the two other timelines also have. See picture below. </p> <p>Then we determine for each interval of the newly created timeline which parts of the previous 2 timelines should be referenced.</p> <p>Note</p> <p>It is good to note that an interval of the newly generated timeline is always smaller than or equal to the interval that was in the original timelines. </p> <p>Note</p> <p>It is also good to remember that since we  have created the new nodes from the old nodes, the new node always corresponds to a node in one of the original timelines.</p> <p>By inspecting the picture we can easily come up with the result:</p> C-start C-end A-start A-end B-start B-end 0 2 0 2 0 2 2 4 2 5 2 4 4 5 2 5 4 7 5 6 5 6 4 7 6 7 6 10 4 7 7 10 6 10 7 10"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#rules","title":"Rules","text":"<p>Now we have the results, we can now reverse-engineer the rules to follow for connecting the correct parts of the original timeline with the new timeline. At first thought you might get the following 3 rules, when either one of them is true then there should be a match. In these set of rules we try to match C with B:</p> <ul> <li>The begin nodes are the same<ul> <li>C-start is equal to B-start (0-2), (2-4), (7-10)</li> </ul> </li> <li>The end nodes are the same<ul> <li>C-end is equal to B-end (0-2), (2-4), (7-10)</li> </ul> </li> <li>The new interval lies completely within the interval of the original<ul> <li>C-start (5-6) is larger than B-start and B-end is smaller than A-end </li> </ul> </li> </ul> <p>However, these rules can be simplified by taking into account the notes we have made earlier.</p> <p>Tip</p> <p>We do not need to check for the end node!</p> <p>The reason that we do not need to check for the end node is because the nodes of timeline C was generated from A and B. This means that it is never going to overshoot a range from one of the originals. So if the begin point of C is within a range of A, then by definition the end point of C will also be in the same range of A. </p> <p>Conclusion</p> <p>So the only check we need to perform per interval of C, is that C-start is  between A-start and A-end. Depending on how your timelines are setup (whether the end point of  the previous record is the same as the start of the next) you might want to  exclude or include your between filter.</p>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#rules-for-immediately-closed-records","title":"Rules for immediately closed records","text":"<p>In case there are records that have the exact same begin and end, and you have  confirmed that that is functionally correct and desired, you need to have logic for this special case and for the normal case. For the special case we also need to add an extra record on timeline C with the C-start and C-end equal.</p> <p>To the rule for the normal case we need to add that the begin and end are not coinciding.</p> <p>Tip</p> <p>If you are certain that records with begin and end times equal only occur at the end of your timeline. Then you can simplify the logic by not having to add above mentioned filter to your normal case. This is because we only use the start node of the new timeline to do the matching. That means there is never going to be a check on your last node (which can only be an end node).</p> <p>The rules for the special case would be:</p> <ul> <li>C-start equals C-end, and</li> <li>C-start equals A-start, and</li> <li>C-end equals A-end</li> </ul>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#code-implementation","title":"Code implementation","text":"<p>For the code sample we will be using SQL. We will also be using Common Table Expressions to divide up the steps.</p>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#getting-the-nodes","title":"Getting the nodes","text":"<pre><code>WITH CTE_nodes AS\n(\n    SELECT DISTINCT id, start_id            FROM A     \n    UNION        \n    SELECT DISTINCT id, end_id AS start_id  FROM A    \n    UNION\n    SELECT DISTINCT id, start_id            FROM B     \n    UNION        \n    SELECT DISTINCT id, end_id AS start_id  FROM B\n)\n</code></pre>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#generating-the-new-timeline","title":"Generating the new timeline","text":"<p>Here we assume that you are using a SQL dialect that supports <code>QUALIFY</code> such as snowflake and databricks. If the SQL dialect does not have this functionality, you would need to add an extra CTE for filtering.</p> <p><pre><code>,CTE_timelines AS\n(\n    SELECT \n        id\n    ,   start_id\n    ,   LEAD(start_id) OVER (PARTITION BY id ORDER BY start_id) AS end_id\n    FROM CTE_nodes\n    QUALIFY end_id IS NOT NULL\n)\n</code></pre> We do not want the last row, since we put the start and endpoints in the same list. If you have the special case where the start and end can be the same, we need to manually append it to the generated timeline.</p> special case<pre><code>,CTE_timelines AS\n(\n    SELECT \n        id\n    ,   start_id\n    ,   LEAD(start_id) OVER (PARTITION BY id ORDER BY start_id) AS end_id\n    FROM CTE_nodes\n    QUALIFY end_id IS NOT NULL\n\n    UNION\n\n    SELECT id, start_id, end_id FROM A WHERE start_id = end_id\n\n    UNION\n\n    SELECT id, start_id, end_id FROM B WHERE start_id = end_id\n)\n</code></pre>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#attaching-original-records-to-the-new-timeline","title":"Attaching original records to the new timeline","text":"<pre><code>SELECT *\nFROM CTE_timelines AS CTE\n    LEFT JOIN A\n        ON CTE.id = A.id\n        AND CTE.start_id &gt;= A.start_id\n        AND CTE.start_id &lt; A.end_id\n    LEFT JOIN B\n        ON CTE.id = B.id\n        AND CTE.start_id &gt;= B.start_id\n        AND CTE.start_id &lt; B.end_id\n</code></pre> <p>In case we are dealing with the special case, we have:</p> Special case<pre><code>SELECT *\nFROM CTE_timelines AS CTE\n    LEFT JOIN A\n        ON CTE.id = A.id\n        AND (\n            (CTE.start_id &gt;= A.start_id AND CTE.start_id &lt; A.end_id AND CTE.start_id &lt;&gt; CTE.end_id AND A.start_id &lt;&gt; A.end_id)\n            OR\n            (CTE.start_id = CTE.end_id AND CTE.start_id = A.start_id AND CTE.end_id = A.end_id)\n        )\n    LEFT JOIN B\n        ON CTE.id = B.id\n        AND (\n            (CTE.start_id &gt;= B.start_id AND CTE.start_id &lt; B.end_id AND CTE.start_id &lt;&gt; CTE.end_id AND B.start_id &lt;&gt; B.end_id)\n            OR\n            (CTE.start_id = CTE.end_id AND CTE.start_id = B.start_id AND CTE.end_id = B.end_id)\n        )\n</code></pre>"},{"location":"Data_Engineering_Analysis/general/timeline_merging/#full-code-example","title":"Full code example","text":"<p>The following example can be run in Snowflake, just copy and paste. For the special case we have added an additional example.</p> Normal caseSpecial case <p><pre><code>CREATE DATABASE IF NOT EXISTS Timelines;\nUSE DATABASE Timelines;\n\nCREATE OR REPLACE TEMPORARY TABLE A AS\nSELECT COLUMN1 AS id, COLUMN2 AS start_id, COLUMN3 AS end_id\nFROM\n    VALUES\n        (1,0,2)\n    ,   (1,2,5)\n    ,   (1,5,6)\n    ,   (1,6,10);\n\nCREATE OR REPLACE TEMPORARY TABLE B AS\nSELECT COLUMN1 AS id, COLUMN2 AS start_id, COLUMN3 AS end_id\nFROM\n    VALUES\n        (1,0,2)\n    ,   (1,2,4)\n    ,   (1,4,7)\n    ,   (1,7,10);\n\n\nWITH CTE_nodes AS\n(\n    SELECT DISTINCT id, start_id            FROM A\n    UNION\n    SELECT DISTINCT id, end_id AS start_id  FROM A\n    UNION\n    SELECT DISTINCT id, start_id            FROM B\n    UNION\n    SELECT DISTINCT id, end_id AS start_id  FROM B\n),CTE_timelines AS\n(\n    SELECT\n        id\n    ,   start_id\n    ,   LEAD(start_id) OVER (PARTITION BY id ORDER BY start_id) AS end_id\n    FROM CTE_nodes\n    QUALIFY end_id IS NOT NULL\n)\nSELECT\n    CTE.ID, CTE.START_ID, CTE.END_ID\n,   A.ID AS A_ID, A.START_ID AS A_START, A.END_ID AS A_END\n,   B.ID AS B_ID, B.START_ID AS B_START, B.END_ID AS B_END\nFROM CTE_timelines AS CTE\n    LEFT JOIN A\n        ON CTE.id = A.id\n        AND CTE.start_id &gt;= A.start_id\n        AND CTE.start_id &lt; A.end_id\n    LEFT JOIN B\n        ON CTE.id = B.id\n        AND CTE.start_id &gt;= B.start_id\n        AND CTE.start_id &lt; B.end_id;\n</code></pre> </p> <p><pre><code>CREATE DATABASE IF NOT EXISTS Timelines;\nUSE DATABASE Timelines;\n\nCREATE OR REPLACE TEMPORARY TABLE A AS\nSELECT COLUMN1 AS id, COLUMN2 AS start_id, COLUMN3 AS end_id\nFROM\n    VALUES\n        (1,0,2)\n    ,   (1,2,5)\n    ,   (1,5,6)\n    ,   (1,6,10)\n    ,   (2,0,10)\n    ,   (2,10,10);\n\nCREATE OR REPLACE TEMPORARY TABLE B AS\nSELECT COLUMN1 AS id, COLUMN2 AS start_id, COLUMN3 AS end_id\nFROM\n    VALUES\n        (1,0,2)\n    ,   (1,2,4)\n    ,   (1,4,7)\n    ,   (1,7,10)\n    ,   (2,0,8)\n    ,   (2,8,8)\n    ,   (2,8,10)\n    ,   (2,10,10);\n\n\nWITH CTE_nodes AS\n(\n    SELECT DISTINCT id, start_id            FROM A\n    UNION\n    SELECT DISTINCT id, end_id AS start_id  FROM A\n    UNION\n    SELECT DISTINCT id, start_id            FROM B\n    UNION\n    SELECT DISTINCT id, end_id AS start_id  FROM B\n),CTE_timelines AS\n(\n    SELECT\n        id\n    ,   start_id\n    ,   LEAD(start_id) OVER (PARTITION BY id ORDER BY start_id) AS end_id\n    FROM CTE_nodes\n    QUALIFY end_id IS NOT NULL\n\n    UNION\n\n    SELECT id, start_id, end_id FROM A WHERE start_id = end_id\n\n    UNION\n\n    SELECT id, start_id, end_id FROM B WHERE start_id = end_id\n)\nSELECT\n    CTE.ID, CTE.START_ID, CTE.END_ID\n,   A.ID AS A_ID, A.START_ID AS A_START, A.END_ID AS A_END\n,   B.ID AS B_ID, B.START_ID AS B_START, B.END_ID AS B_END\nFROM CTE_timelines AS CTE\n    LEFT JOIN A\n        ON CTE.id = A.id\n        AND (\n            (CTE.start_id &gt;= A.start_id AND CTE.start_id &lt; A.end_id AND CTE.start_id &lt;&gt; CTE.end_id AND A.start_id &lt;&gt; A.end_id)\n            OR\n            (CTE.start_id = CTE.end_id AND CTE.start_id = A.start_id AND CTE.end_id = A.end_id)\n        )\n    LEFT JOIN B\n        ON CTE.id = B.id\n        AND (\n            (CTE.start_id &gt;= B.start_id AND CTE.start_id &lt; B.end_id AND CTE.start_id &lt;&gt; CTE.end_id AND B.start_id &lt;&gt; B.end_id)\n            OR\n            (CTE.start_id = CTE.end_id AND CTE.start_id = B.start_id AND CTE.end_id = B.end_id)\n        )\nORDER BY CTE.ID, CTE.START_ID, CTE.END_ID\n</code></pre> </p>"},{"location":"Data_Engineering_Analysis/spark/","title":"Spark","text":""},{"location":"Data_Engineering_Analysis/spark/output_file_count/","title":"Output file count","text":"<p>The normal behaviour is that every executor writes its own files. How many files each executor writes depends on how many tasks there were to be picked up. Although you could force 1 output file, it will negatively impact the performance. The data will be shuffled to 1 executor.</p> <p>There are 3 ways to influence the amount of output files:</p> <ol> <li><code>spark.default.parallelism</code></li> <li><code>spark.sql.shuffle.partitions</code></li> <li><code>repartition()</code> or <code>coalesce()</code></li> </ol> <p>For more information, see https://www.google.com/amp/s/hadoopsters.com/2019/06/22/how-to-control-file-count-reducers-and-partitions-in-spark-and-spark-sql/amp/.</p>"},{"location":"Data_Engineering_Analysis/spark/pyspark_on_windows/","title":"Install Pyspark on Windows","text":""},{"location":"Data_Engineering_Analysis/spark/pyspark_on_windows/#pyspark","title":"Pyspark","text":"<p>Pyspark can be installed from pip:</p> <pre><code>pip install pyspark\n</code></pre>"},{"location":"Data_Engineering_Analysis/spark/pyspark_on_windows/#hadoop","title":"Hadoop","text":"<p>Pyspark also relies on Hadoop. The required binaries can be found in the repo  https://github.com/cdarlint/winutils. </p> <p></p> <p>Download the desired Hadoop version and set the correct environment variables.</p> <pre><code>HADOOP_HOME=&lt;your local hadoop-ver folder&gt;\nPATH=%PATH%;%HADOOP_HOME%\\bin\n</code></pre>"},{"location":"Data_Engineering_Analysis/spark/pyspark_on_windows/#python","title":"Python","text":"<p>At this stage pyspark might work, but there is a big chance that it is complaining about not finding <code>python3.exe</code>. There are 2 things you need to do:</p> <ol> <li> <p>Check whether Windows has registered any aliases for the keywords <code>Python</code> and <code>Python3</code></p> <ol> <li> <p>Open the start menu and search for <code>Manage app execution aliases</code></p> <p></p> </li> <li> <p>Then for <code>Python</code> and <code>Python3</code> turn the toggles off</p> <p></p> </li> </ol> </li> <li> <p>Check whether there is an executable <code>python3.exe</code> in your python installation directory</p> <ol> <li>Find the directories by typing<ol> <li><code>cmd</code>: <code>where python</code></li> <li><code>Powershell</code>: <code>Get-Command python</code> or <code>gcm python</code> </li> </ol> </li> <li>If there is no <code>python3.exe</code> present. Copy paste <code>python.exe</code> and rename it to <code>python3.exe</code></li> </ol> </li> </ol> <p>After performing these 2 steps, restart the command window. After that pyspark should be working.</p>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/","title":"Recommended spark configuration","text":"<p>Spark has an enormous set of configuration that can be changed, see the documentation  for the list.</p>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#recommendation-by-apache","title":"Recommendation by Apache","text":"<p>Please see the link: https://spark.apache.org/docs/latest/cloud-integration.html</p>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#some-highlights","title":"Some highlights","text":"<p>Spark first writes the output files to a temporary \"folder\". When everything is finished, it renames all the files. This renaming is having a great hit on performance. A solution to limit the impact is to use the fileoutputcommitter algorithm version 2.</p> <pre><code>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: 2\n</code></pre> <p>Version 1 is safer, but slower. According to the link, the consistency models for AWS and Azure are consistent, as of 2021. So this should not be an issue.</p> <p>Quote</p> <p>As of 2021, the object stores of Amazon (S3), Google Cloud (GCS) and Microsoft  (Azure Storage, ADLS Gen1, ADLS Gen2) are all consistent.</p>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#other-recommendations","title":"Other recommendations","text":""},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#adaptive-query-execution","title":"Adaptive Query Execution","text":"<p>Adaptive Query Execution (AQE) not only evaluates the execution at the start, but as it gets more insight in the  data, it will change the query plan accordingly. </p> <p>Starting with spark 3.2 AQE is enabled by default. Also, in spark 3.2 the skew-joins are handles much better.</p> <pre><code>spark.sql.adaptive.enabled: True\nspark.sql.adaptive.coalescePartitions.enabled: True\nspark.sql.adaptive.skewJoin.enabled: True\n</code></pre> <p>Further reading: https://sparkbyexamples.com/spark/spark-adaptive-query-execution/</p>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#speculation","title":"Speculation","text":"<p>With speculation enabled the engine tries to predict what it might need to do next when there are executors idling. This is the same what your CPU is doing all the time. As a consequence, you may see in the Spark UI that tasks get killed because of speculation. All this means is that the task is already done by another executor.</p> <pre><code>spark.speculation: True\n</code></pre>"},{"location":"Data_Engineering_Analysis/spark/recommended_spark_conf/#compression-codec","title":"Compression codec","text":"<p>Spark offers 4 compression standards:</p> <ul> <li><code>lz4</code></li> <li><code>lzf</code></li> <li><code>snappy</code></li> <li><code>zstd</code></li> </ul> <p>The compression method <code>ZSTD</code> is going to be default in spark: https://issues.apache.org/jira/browse/SPARK-35181. Or atleast, the intent is there. <code>ZSTD</code> should be faster and more efficient than <code>snappy</code>.</p> <pre><code>spark.io.compression.codec: ZSTD\n</code></pre>"},{"location":"Data_Engineering_Analysis/spark/spark_session/","title":"Spark session","text":"<p>The spark session is the entry point to programming Spark with the Dataset and DataFrame API. It is convention to put the spark session in a variable called <code>spark</code>.</p> <p>See the documentation:</p> <p>https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html</p> <p>To create the spark session, you can make use of the spark session builder. Creating the session does not require any arguments, and can be as simple as:</p>"},{"location":"Data_Engineering_Analysis/spark/spark_session/#the-spark-builder","title":"The Spark Builder","text":"<pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n</code></pre> <p>Although arguments are not necessary, there are some useful settings that can be set.</p> <ul> <li><code>master</code> <ul> <li>Sets the Spark master URL to connect to, such as \u201clocal\u201d to run locally,    \u201clocal[4]\u201d to run locally with 4 cores, or \u201cspark://master:7077\u201d to run on    a Spark standalone cluster.</li> </ul> </li> <li><code>appName</code><ul> <li>Sets a name for the application, which will be shown in the Spark web UI.</li> </ul> </li> <li><code>config</code><ul> <li>Sets a config. The input is a list of strings, alternating between key name and value.   Or you can chain the config multiple times.</li> </ul> </li> </ul> <p>When all config is set, you can call <code>getOrCreate</code> on the builder to create the spark session.</p> <p>In the example below we create a spark session on our local machine using 4 cores. We also allocate 4 gigabytes of memory to the driver. </p> <pre><code>from pyspark.sql import SparkSession\n\n# Specify the amount of cores\ncores: int = 4\n\n# Specify the amount of memory for the driver\ndriver_memory = \"4g\"\n\n# Give the app a name\napp_name = \"example\"\n\nbuilder = (\n    SparkSession.builder.master(f\"local[{cores}]\")\n    .appName(app_name)\n    .config(\"spark.driver.memory\", driver_memory)\n)\n\nspark = builder.getOrCreate()\n</code></pre>"},{"location":"Data_Engineering_Analysis/spark/spark_session/#delta-tables","title":"Delta tables","text":"<p>To use delta tables with spark, we need to install the package <code>delta-spark</code>. https://delta.io/learn/getting-started/</p> <p>The easiest way to work with delta, spark and python is to add some config to the  spark builder. </p> <ul> <li><code>spark.sql.extensions</code>: <code>io.delta.sql.DeltaSparkSessionExtension</code></li> <li><code>spark.sql.catalog.spark_catalog</code>: <code>org.apache.spark.sql.delta.catalog.DeltaCatalog</code></li> </ul> <p>Then we also need to download and configure the correct delta-core package. This  depends on which scala version and spark version you are on. Luckily, there is a  function provided by delta that does this for us (<code>configure_spark_with_delta_pip</code>).  We just need to pass the builder to the function, and it will add those packages  to the builder.</p> <p>To not have to type this everytime we need spark, a simple function is created:</p> <pre><code>from pyspark.sql import SparkSession\n\n\ndef get_or_create_spark_session(\n    use_delta: bool = True,\n    cores: int = 4,\n    app_name: str = \"Local\",\n    driver_memory: str = \"4g\",\n):\n    # If a spark session already exists, do not build a new one\n    if spark := SparkSession.getActiveSession():\n        return spark\n\n    builder = (\n        SparkSession.builder.master(f\"local[{cores}]\")\n        .appName(app_name)\n        .config(\"spark.driver.memory\", driver_memory)\n    )\n\n    if use_delta:\n        from delta import configure_spark_with_delta_pip\n\n        builder = configure_spark_with_delta_pip(\n            builder.config(\n                \"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n            ).config(\n                \"spark.sql.catalog.spark_catalog\",\n                \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n            )\n        )\n\n    return builder.getOrCreate()\n</code></pre>"},{"location":"Productivity/","title":"Productivity Tools","text":"<p>In this section we cover productivity tools or office tools.</p>"},{"location":"Productivity/browsers/","title":"Browsers","text":""},{"location":"Productivity/browsers/customise_css/","title":"Customise CSS on any webpage","text":"<p>You can inject and overwrite the css of any website using the chrome extensions Stylish  or Stylus. The first one had some privacy issues in the past, so it is advised to use Stylus.</p> <p>You can write your own css or you can make use of the marketplace userstyles.org.</p> <p>In the next section some handy userstyles will be mentioned.</p>"},{"location":"Productivity/browsers/customise_css/#pepper-grid-mode","title":"Pepper grid mode","text":"<p>Since Pepper does not support grid mode anymore, it makes going over the deals a lot harder. In order to bring it the back, the following userstyle can be used: Pepper Grid Mode.</p> <p>In case the userstyles site loads slow, here is the css code:</p> Pepper Grid Mode code <pre><code>@-moz-document url-prefix(\"https://www.mydealz.de/\"), url-prefix(\"https://www.promodescuentos.com/\"), url-prefix(\"https://www.pelando.com.br/\"), url-prefix(\"https://www.hotukdeals.com/\"), url-prefix(\"https://www.dealabs.com/\"), url-prefix(\"https://nl.pepper.com/\"), url-prefix(\"https://www.preisjaeger.at/\"), url-prefix(\"https://www.pepper.pl/\"), url-prefix(\"https://www.chollometro.com/\"), url-prefix(\"https://www.pepper.ru/\"), url-prefix(\"https://www.pepper.it/\") {\n/*\n\n# GRID MODE #\nBrings back grid mode to Pepper\n\n# COMPATIBLE WITH #\nhttps://www.pepper.pl/\nhttps://www.mydealz.de/\nhttps://www.promodescuentos.com/\nhttps://www.pelando.com.br/\nhttps://www.hotukdeals.com/\nhttps://www.dealabs.com/\nhttps://nl.pepper.com/\nhttps://www.preisjaeger.at/\nhttps://www.chollometro.com/\nhttps://www.pepper.ru/\nhttps://www.pepper.it/\n\n*/\n .cept-thread {\n    display: block !important;\n    width: 100% !important;\n    margin: 0 !important;\n    padding: 0 !important;\n}\n.listLayout {\n    max-width: 108em;\n}\n.listLayout-side {\n    max-width: 280px;\n}\n.listLayout-main .thread {\n    display: none;\n    position: relative;\n    width: 230px;\n    margin: 4px 2.5px;\n    text-align: left;\n    padding-top: 44px;\n    padding-bottom: 48px;\n}\n.cept-thread-item {\n    display: inline-flex !important;\n}\n.threadGrid {\n    display: block !important;\n    width: 100%;\n    padding: 0px;\n}\n.threadGrid-image {\n    float: unset;\n    display: block;\n    max-height: 200px;\n    position: relative;\n}\n.threadGrid-image .imgFrame {\n    position: unset;\n}\n.threadGrid-image .imgFrame img {\n    max-height: 200px;\n}\n.tGrid-row .listLayout {\n    text-align: center;\n}\n.threadGrid .footerMeta .iGrid-item {\n    display: block;\n}\n.threadGrid-body .flex--fromW2 {\n    display: block;\n}\n.threadGrid-body .flex--fromW2 .width--fromW2-6 {\n    width: 100%;\n}\n.threadGrid-body .hide--empty {\n    display: none;\n}\n.threadGrid-body .flex--fromW2 .btn--mode-primary {\n}\n.threadGrid-headerMeta {\n    position: absolute;\n    top: 5px;\n    left: 5px;\n    width: calc(100% - 10px);\n}\n.threadGrid .footerMeta-actionSlot button.cept-off, .threadGrid .footerMeta-actionSlot button.cept-on {\n    display: none;\n}\n.threadGrid .footerMeta, .threadGrid .iGrid-item {\n    position: unset;\n}\n.threadGrid .cept-dealBtn, .threadGrid .cept-tb {\n    position: absolute;\n    bottom: 5px;\n    left: 5px;\n    width: calc(100% - 10px);\n}\n.threadGrid .cept-description-container {\n    text-overflow: ellipsis;\n    overflow: hidden;\n    height: 32px;\n    max-height: 32px;\n    line-height: 1rem;\n    font-size: .75rem;\n    word-break: break-word;\n    display: -webkit-box;\n    -webkit-line-clamp: 2;\n    -webkit-box-orient: vertical;\n    margin: 6.5px 0px;\n}\n.threadGrid .thread-title {\n    margin-bottom: 8px;\n}\n.threadGrid .thread-title .cept-tt {\n    font-size: .875rem;\n    display: block;\n    height: 40px;\n    max-height: 40px;\n    line-height: 1.25rem;\n}\n.threadGrid .cept-shipping-price .hide--toW3 {\n    display: none !important;\n}\n.threadGrid .cept-shipping-price .hide--fromW3 {\n    display: block !important;\n}\n.threadGrid .threadGrid-title .thread-price, .threadGrid .threadGrid-title .overflow--fade .flex--inline .mute--text, .threadGrid .threadGrid-title .overflow--fade .flex--inline .space--ml-1 {\n    font-size: 1rem;\n    line-height: 1.5rem;\n}\n.threadGrid .threadGrid-title .cept-merchant-name {\n    line-height: 1.5rem;\n}\n.threadGrid-headerMeta .metaRibbon.hide--toW2 {\n    display: none;\n}\n.threadGrid-headerMeta .hide--toW3 {\n    display: none;\n}\n.threadGrid-headerMeta .hide--fromW3 {\n    display: block !important;\n}\n.threadGrid .threadGrid-title {\n    padding: 0px 8px;\n}\n.threadGrid .threadGrid-body {\n    padding: 0px 6px;\n}\n.threadGrid .threadGrid-footerMeta {\n    display: block;\n}\n.threadGrid-footerMeta .footerMeta-infoSlot {\n    margin-left: 8px;\n}\n.threadGrid .threadGrid-image {\n    margin: 8px 0px;\n}\n.subNavMenu-list .cept-layoutBtn-list {\n    border-radius: 8px;\n    border: 2px solid #eee;\n    padding: 0px 8px 0px 0px;\n}\n.subNavMenu-list .cept-layoutBtn-list:after {\n    content:'GRID';\n}\n.subNavMenu-list .cept-layoutBtn-text {\n    display: none;\n}\n/* Hide banners and widgets on mydealz */\n .listLayout-main .cept-banner {\n    display: none;\n}\n.listLayout-main .cept-widget-list {\n    display: none;\n}\n.threadGrid .threadGrid-body .flex--fromW2 .cept-vcb {\n    display: none;\n}\n.threadGrid-footerMeta .footerMeta-actionSlot {\n    box-shadow: -10px 0 10px -3px #fff;\n    z-index: 1;\n}\n.boxSec-div.comments-list {\n    width: 320px;\n    position:absolute;\n    height: 600px;\n    overflow: auto;\n    top: 140px;\n    background-color: #fff;\n    z-index: 1000;\n    border: 2px solid #bfbfbf;\n    border-radius: 10px;\n    box-shadow: 0 0 9px 0px #bfbfbf;\n}\n.boxSec-div.comments-list img.avatar, .boxSec-div.comments-list .comment-footer {\n    display: none;\n}\n.footerMeta-actionSlot .cept-comment-link.btn--mode-primary {\n    /* hide comments button */\n}\n.threadGrid-headerMeta .cept-thread-group-name {\n    -webkit-line-clamp: 2;\n    max-height: 40px;\n    line-height: 1.25rem;\n    display: -webkit-box;\n    word-break: break-word;\n    text-overflow: ellipsis;\n    overflow: hidden;\n}\n.listLayout article.thread--discussion {\n}\n.listLayout article.thread--discussion .threadGrid .cept-description-container {\n    height: 120px !important;\n    max-height: 120px !important;\n}\n.listLayout article.thread--discussion .threadGrid .cept-vote-box {\n    display: none;\n}\n.thread-listDiscountCell {\n    display: none;\n}\n}\n</code></pre>"},{"location":"Productivity/browsers/inject_js/","title":"Inject javascript on any webpage","text":"<p>For this you can make use of Greasemonkey  on firefox and TamperMonkey on chromium based browsers.</p>"},{"location":"Productivity/flash/flashpoint/","title":"BlueMaxima's Flashpoint","text":"<p>Flash has been end of life since 2021. However, there were lots of nice flash games. To preserve them and to play them. Bluemaxima has created a collection of flash games and created an application that allows you to play the flash games.</p> <p>Please see: https://bluemaxima.org/flashpoint/</p>"},{"location":"Productivity/general/","title":"General","text":"<p>These are pages that cannot be categorised easily.</p>"},{"location":"Productivity/general/RTHK_archive/","title":"Record RTHK Archive","text":"<p>For this you will need the tool FFMPEG and the link to the playlist file.</p>"},{"location":"Productivity/general/RTHK_archive/#playlist-file-link","title":"Playlist file link","text":"<p>In order to get the playlist link, you need to first navigate to the page of the programme. For example: https://www.rthk.hk/radio/radio1/programme/Hongkongstory. Once you are on the page, you need to make sure that a player is present (see image below). </p> <p></p> <p>If there is no player available, then you have to click on one of the episodes so a player will appear.</p> <p></p> <p>Then what you need to do is Right click and then view page source. The part you need to look for is called <code>jw audio player</code>.  There you will find the file link.</p>"},{"location":"Productivity/general/RTHK_archive/#record-using-ffmpeg","title":"Record using ffmpeg","text":"<p>Navigate to the folder where the ffmpeg.exe file is. Then open a command window in that folder.</p> <pre><code>ffmpeg -i &lt;file_link&gt; &lt;name_of_downloaded_file&gt;.mp3\n</code></pre>"},{"location":"Productivity/general/RTHK_archive/#create-command-file-for-a-specific-program","title":"Create command file for a specific program","text":"<p>If you want to record a programme more often, it might come in handy to create a bat file for it.</p> Download for specific date<pre><code>@ECHO OFF\nSET /p datefile=Date in the format (yyyymmdd)\n:Label1\nffmpeg -i https://rthkaod3-vh.akamaihd.net/i/m4a/radio/archive/radio1/Hongkongstory/m4a/%datefile%.m4a/master.m3u8 %datefile%.mp3\nfor /F %%A in (\"%datefile%.mp3\") do set size=%%~zA\nset /a KB=(%size%/1024)\nIF %KB% LSS 16000 (del %datefile%.mp3 &amp; GOTO Label1)\npause\n</code></pre> Download for today<pre><code>@echo off\nfor /f \"tokens=2 delims==\" %%a in ('wmic OS Get localdatetime /value') do set \"dt=%%a\"\nset \"YY=%dt:~2,2%\" &amp; set \"YYYY=%dt:~0,4%\" &amp; set \"MM=%dt:~4,2%\" &amp; set \"DD=%dt:~6,2%\"\nset \"HH=%dt:~8,2%\" &amp; set \"Min=%dt:~10,2%\" &amp; set \"Sec=%dt:~12,2%\"\n\nset \"datefile=%YYYY%%MM%%DD%\" \n\n:Label1\nffmpeg -i https://rthkaod3-vh.akamaihd.net/i/m4a/radio/archive/radio1/Hongkongstory/m4a/%datefile%.m4a/master.m3u8 %datefile%.mp3\nfor /F %%A in (\"%datefile%.mp3\") do set size=%%~zA\nset /a KB=(%size%/1024)\nIF %KB% LSS 16000 (del %datefile%.mp3 &amp; GOTO Label1)\npause\n</code></pre> <p>The check for file size was necessary for older versions of ffmpeg and is probably not needed anymore, as the tool has become a lot more stable.</p>"},{"location":"Productivity/general/download_online_videos/","title":"Download online videos","text":""},{"location":"Productivity/general/download_online_videos/#youtube-dl","title":"youtube-dl","text":"<p>Attention</p> <p>2022-Feb: There seems to be something wrong with youtube-dl as the download speeds are way down.</p> <p>The easiest way to download a video from one of the popular sites such as Youtube and Vimeo is to use youtube-dl (link to github).  Although the name suggests that it can only download from Youtube, the contrary is true. For the full list please visit: https://ytdl-org.github.io/youtube-dl/supportedsites.html.</p>"},{"location":"Productivity/general/download_online_videos/#download-and-installing","title":"Download and installing","text":"<p>There are multiple ways to install youtube-dl. You can download the compiled binary (.exe) or you can use pip to install it, as it is written in python. Downloading the exe file does not require installing.</p>"},{"location":"Productivity/general/download_online_videos/#basic-syntax","title":"Basic syntax","text":"<p>The most simple syntax is by just passing the URL to the program.</p> <pre><code>youtube-dl &lt;your-URL&gt;\n</code></pre>"},{"location":"Productivity/general/download_online_videos/#pair-with-ffmpeg","title":"Pair with ffmpeg","text":"<p>If you want to have some format conversion options, you can pair youtube-dl with ffmpeg. You just need to make sure that either the binaries are in the same folder, or that both are registered in the environment variables.</p> <p>Then you can for example do the following to extract the audio as mp3.</p> <pre><code>@ECHO OFF\nSET /p Link=YoutubeLink (Rechtermuisknop): \nbin\\youtube-dl.exe -f bestaudio --audio-quality 0 --prefer-ffmpeg --extract-audio --audio-format mp3 %Link%\n</code></pre>"},{"location":"Productivity/general/download_online_videos/#videocacheview","title":"VideoCacheView","text":"<p>If youtube-dl does not work you can always try to retrieve the video from the browser cache. If you can view something, you should be able to download it. To retrieve videos (and also other media) from cache you can use VideoCacheView.</p>"},{"location":"Productivity/general/download_online_videos/#how-to-use","title":"How to use","text":"<p>When first opening the application it should show you some options you can set. If the dialog does not  show up, you can go to the menu bar -&gt; Options -&gt; Advanced Options. In there, make sure that your browser is ticked.</p> <p></p> <p>Then navigate in your browser to the video and let it buffer.</p> <p>Tip</p> <p>You can speed up the process by manually skipping to the end of what has been buffered.</p> <p>While buffering you can keep an eye on the VideoCacheView window. The file names are a bit cryptic, but if you sort on last accessed or file size you should be able to easily determine which file is yours. Also because the size is increasing as it buffers, that is also an easy hint to the correct file.</p> <p></p> <p>Select the file you want and then press the second button from the left to copy the files. In the next dialogue you can choose the destination. You will see 2 files, 1 containing the audio stream and the other containing the video stream. The outputs can either be <code>.mp4</code> format or <code>.webm</code>. </p> <p></p> <p>Note</p> <p>When the format is <code>.mp4</code> VideoCacheView will attempt to merge them using a library called GPAC. If you do not have that library installed you will get  the 2 separate files.</p>"},{"location":"Productivity/general/download_online_videos/#combining-video-and-audio","title":"Combining video and audio","text":"<p>To combine the video and audio, you can make use of ffmpeg. See the following post on superuser for the source: https://superuser.com/questions/277642/how-to-merge-audio-and-video-file-in-ffmpeg.</p> <pre><code>ffmpeg -i video.mp4 -i audio.wav -c:v copy -c:a aac output.mp4\n</code></pre> <p>In this example the video is not re-encoded but the audio is. In case re-encoding is not  necessary at all you can use:</p> <pre><code>ffmpeg -i video.mp4 -i audio.wav -c copy output.mkv\n</code></pre>"},{"location":"Productivity/general/overscan/","title":"Overscan on TV's","text":"<p>Sometimes you see that a part of your screen is missing if you plug in your PC. This phenomenon is called overscan. This was used in older CRT TV's to make the most important part of the view fit. </p> <p>On fixed pixel devices such as modern TV's this is not needed anymore. For more information see this article on howtogeek.</p> <p>You can resolve this by setting the picture settings on your TV. https://leapfrog.happyfox.com/kb/article/267-2993/</p> <p>For LG you have to look for <code>Just Scan</code> in the aspect ratio menu. </p>"},{"location":"Productivity/ms_office/outlook/filter_on_message_type/","title":"Filter on message type","text":"<p>Sometimes it is handy to filter on meeting invites for example. In the desktop version of Outlook you can do that.</p> <p>Source: </p> <ul> <li>https://superuser.com/questions/555304/how-to-search-for-item-type-in-outlook</li> <li>Link to exact post: https://superuser.com/a/713419</li> </ul> <p>Full answer from hotshot309</p> <p>In the search box, include <code>messageclass:IPM.Note</code> to find only e-mail messages.</p> <p>Here is the full list (as far as I know) of message classes for Outlook/Exchange 2010 items.  I found some of this information in Mastering Microsoft Exchange Server 2010  by Jim McBee and David Elfassy.  I found the rest by adding the \"Message Class\" column to an Outlook folder view and noting the values for different types of messages in that folder.</p> <ul> <li>IPM.Activity: Item in Outlook Journal folder</li> <li>IPM.Appointment: Item on Outlook Calendar</li> <li>IPM.Contact: Item in Outlook Contacts folder</li> <li>IPM.Document: Document/file (I don't believe this filter would ever yield results when searching Outlook e-mail)</li> <li>IPM.Note: E-mail message</li> <li>IPM.Note.Rules.OofTemplate.Microsoft: Received out-of-office auto reply e-mail</li> <li>IPM.Note.SMIME: Encrypted or digitally signed e-mail message</li> <li>IPM.Note.Microsoft.Conversation: Office Communication Server instant message or group chat session</li> <li>IPM.Note.Microsoft.Conversation.Voice: Office Communication Server call</li> <li>IPM.Note.Microsoft.Fax: Fax message</li> <li>IPM.Note.Microsoft.Missed.Voice: Notice of missed Office Communication Server call</li> <li>IPM.Post: Internet post item, usually for a public folder (I am unfamiliar with this one)</li> <li>IPM.Post.RSS: RSS feed</li> <li>IPM.Schedule: Meeting-related notice (will catch all of the specific types below)</li> <li>IPM.Schedule.Meeting.Canceled: Meeting cancellation notice</li> <li>IPM.Schedule.Meeting.Notification.Forward: Forwarded meeting invitation</li> <li>IPM.Schedule.Meeting.Request: New or updated received meeting invitation</li> <li>IPM.Schedule.Meeting.Resp.Neg: Sent declined meeting notice</li> <li>IPM.Schedule.Meeting.Resp.Pos: Sent accepted meeting notice</li> <li>IPM.Schedule.Meeting.Resp.Tent: Sent tentative meeting notice</li> <li>IPM.StickyNote: Item in Outlook Notes folder</li> <li>IPM.Task: Item in Outlook Tasks folder</li> <li>IPM.Note.Microsoft.Voicemail: Voicemail left via Exchange 2007 Unified Messaging server or equivalent</li> <li>IPM.Sharing: The Sharing Message Object Protocol is used to share mailbox folders between clients. This protocol extends the Message and Attachment Object Protocol.</li> </ul>"},{"location":"Productivity/windows/","title":"Windows","text":""},{"location":"Productivity/windows/custom_volume_mixer/","title":"Custom Volume Mixer","text":"<p>The standard volume mixer in Windows 10 and 11 is not always practical. You need to click through some menus to get to the per application volume (the volume mixer). Also it is not easy to assign one speaker to a  specific application.</p> <p>With EarTrumpet this should be possible. </p>"},{"location":"Productivity/windows/record_screen/","title":"Record your screen","text":""},{"location":"Productivity/windows/record_screen/#open-broadcaster-software","title":"Open Broadcaster Software","text":"<p>Open Boradcaster Software (OBS) is an open source software to record or stream your screen.</p>"},{"location":"Productivity/windows/record_system_audio/","title":"Record System Audio","text":"<p>The simplest way to record the system audio is by using the program Audacity. In the top left corner you need to select <code>WASAPI</code> as the input. Then all you need to do is press the record button.</p> <p></p>"},{"location":"Productivity/windows/windows_photo_viewer/","title":"Bring back Windows Photo Viewer","text":"<p>If you are used to Windows Photo Viewer for opening images, then you can bring that back in Windows 10.</p> <p>https://www.howtogeek.com/225844/how-to-make-windows-photo-viewer-your-default-image-viewer-on-windows-10/</p> <p>Contents of the registry file: Activate Windows Photo Viewer on Windows 10.reg<pre><code>Windows Registry Editor Version 5.00\n\n; created by Walter Glenn\n; for How-To Geek\n; article: https://www.howtogeek.com/225844/how-to-make-windows-photo-viewer-your-default-image-viewer-on-windows-10/\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll]\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell]\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\open]\n\"MuiVerb\"=\"@photoviewer.dll,-3043\"\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\open\\command]\n@=hex(2):25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,6f,00,6f,00,74,00,25,\\\n00,5c,00,53,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,00,5c,00,72,00,75,00,\\\n6e,00,64,00,6c,00,6c,00,33,00,32,00,2e,00,65,00,78,00,65,00,20,00,22,00,25,\\\n00,50,00,72,00,6f,00,67,00,72,00,61,00,6d,00,46,00,69,00,6c,00,65,00,73,00,\\\n25,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,20,00,50,00,68,00,6f,\\\n00,74,00,6f,00,20,00,56,00,69,00,65,00,77,00,65,00,72,00,5c,00,50,00,68,00,\\\n6f,00,74,00,6f,00,56,00,69,00,65,00,77,00,65,00,72,00,2e,00,64,00,6c,00,6c,\\\n00,22,00,2c,00,20,00,49,00,6d,00,61,00,67,00,65,00,56,00,69,00,65,00,77,00,\\\n5f,00,46,00,75,00,6c,00,6c,00,73,00,63,00,72,00,65,00,65,00,6e,00,20,00,25,\\\n00,31,00,00,00\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\open\\DropTarget]\n\"Clsid\"=\"{FFE2A43C-56B9-4bf5-9A79-CC6D4285608A}\"\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\print]\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\print\\command]\n@=hex(2):25,00,53,00,79,00,73,00,74,00,65,00,6d,00,52,00,6f,00,6f,00,74,00,25,\\\n00,5c,00,53,00,79,00,73,00,74,00,65,00,6d,00,33,00,32,00,5c,00,72,00,75,00,\\\n6e,00,64,00,6c,00,6c,00,33,00,32,00,2e,00,65,00,78,00,65,00,20,00,22,00,25,\\\n00,50,00,72,00,6f,00,67,00,72,00,61,00,6d,00,46,00,69,00,6c,00,65,00,73,00,\\\n25,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,20,00,50,00,68,00,6f,\\\n00,74,00,6f,00,20,00,56,00,69,00,65,00,77,00,65,00,72,00,5c,00,50,00,68,00,\\\n6f,00,74,00,6f,00,56,00,69,00,65,00,77,00,65,00,72,00,2e,00,64,00,6c,00,6c,\\\n00,22,00,2c,00,20,00,49,00,6d,00,61,00,67,00,65,00,56,00,69,00,65,00,77,00,\\\n5f,00,46,00,75,00,6c,00,6c,00,73,00,63,00,72,00,65,00,65,00,6e,00,20,00,25,\\\n00,31,00,00,00\n\n[HKEY_CLASSES_ROOT\\Applications\\photoviewer.dll\\shell\\print\\DropTarget]\n\"Clsid\"=\"{60fd46de-f830-4894-a628-6fa81bc0190d}\"\n</code></pre></p>"},{"location":"coding/","title":"Coding","text":""},{"location":"coding/Python/helpful_resources/","title":"Useful resources and links","text":""},{"location":"coding/Python/helpful_resources/#websitesblogs","title":"Websites/blogs","text":""},{"location":"coding/Python/helpful_resources/#videos","title":"Videos","text":""},{"location":"coding/Python/helpful_resources/#youtube-channels","title":"Youtube Channels","text":"<ul> <li>Arjan Codes</li> <li>mCoding</li> </ul>"},{"location":"coding/Python/unittest/","title":"Unit testing","text":"<p>In unit testing the smallest possible unit of a piece of code is tested. This can for example be a function or  a method in a class. In the test you run the function with predefined input, then you compare that input with  an output that you expect and define preferably in a hard-coded way. By hard-coding you make sure that the expected outcome is defined by you alone, and does not depend on some other function.</p> <p>In python there are some unit testing frameworks, such as the built-in <code>unittest</code> and <code>pytest</code>. </p> <p>Ideally you want every part of your function to be tested. If there are branches created in the function by for example if statements, you want each branch to be covered by a unit test. In practice this might not be that easy to achieve. The python library <code>coverage</code> can generate a code coverage report for you and point out which parts of  the code has not been covered by a unit test.</p>"},{"location":"coding/Python/unittest/#testing-frameworks","title":"Testing frameworks","text":""},{"location":"coding/Python/unittest/#unittest","title":"Unittest","text":"<p>The <code>unittest</code> package is the easiest way to start unit-testing.  See official documentation: https://docs.python.org/3/library/unittest.html.</p> <p>Structure-wise it is best to create a folder named <code>test</code> in the root of your repository. Then in this <code>test</code> folder you follow the structure of your source folder and create for each module a test file. So if the path to one of  the modules is <code>source/your_module.py</code>, then the corresponding test should be located at <code>test/source/test_your_module.py</code>.</p> <p>It is important to start your file with prefix <code>test_</code>. Also the test classes and test methods/functions in that file should start with <code>test_</code>. This is so the test framework can identify the tests, as you might have some helper  classes or functions.</p> <p>In your test file start by importing the <code>unittest</code> package:</p> <pre><code>import unittest\n</code></pre> <p>It is best practise to create a corresponding TestClass for every class in the module you want to test. If the module contains functions that are outside a class, you could bundle them together in a generic test class.</p> <p>To initiaite a unittest class you need to inherit from <code>unittest.TestCase</code>.</p> <pre><code>import unittest\n\n\nclass TestYourClass(unittest.TestCase):\n    ...\n</code></pre> <p>By inheriting from the TestCase class you get access to some useful functions to setup before a test and some  compare functions.</p>"},{"location":"coding/Python/unittest/#setup","title":"Setup","text":"<p>The setup can be defined in a method called setUp().  <pre><code>import unittest\n\nclass TestYourClass(unittest.TestCase):\n    def setUp(self) -&gt; None:\n        self.some_test_data = [\"data1\", \"data2\", \"data3\"]\n</code></pre></p> <p>The <code>setUp</code> method is called everytime an unittest method in that class is called. So you do not need to worry about the contents when you modify it in one of your tests.</p>"},{"location":"coding/Python/unittest/#the-unit-test","title":"The unit test","text":"<p>To define a unittest all you need to do is define a class method with a name starting with <code>test_</code>.</p> <p><pre><code>import unittest\n\nclass TestYourClass(unittest.TestCase):\n    def setUp(self) -&gt; None:\n        self.some_test_data = [\"data1\", \"data2\", \"data3\"]\n\n    def test_some_method(self):\n        output = some_method(self.some_test_data)\n\n        expected_output = \"data1\"\n        self.assertEqual(output, expected_output)\n</code></pre> In the test method you run the method you want to test and compare that with what you expect. The comparison can be done by using one of the many assert methods provided by the TestCase class.</p>"},{"location":"coding/Python/unittest/#mocking-imports","title":"Mocking imports","text":"<p>For unittests you want to just test the functionality in the method, in a controlled manner.  If your method or function has external dependencies on for example a database connection, then the outcome of the test is not deterministic. The database might be down, and thus failing the unittest. To counter this, we can temporarily replace the dependency on the database in the test. This is called mocking.  See official documentation to learn more.</p> <p>With a mock you can redefine a return value of the function or class that you are patching. So in the database case  you might change the output of the database query to a dataset that you define yourself.</p> <p>You can mock a testcase by putting a <code>patch</code> decorator around the test method or test class. You can also use the  <code>patch</code> in a context manager. The easiest way is to just put it around the test method.</p> <p><pre><code>import unittest\nfrom unittest import mock\n\n\nclass TestYourClass(unittest.TestCase):\n    @mock.patch(\"path.to.your.module.import_you_want_to_mock\")\n    def test_your_method(self, mock_some_import):\n        ...\n</code></pre> For every patch you define, you will get a corresponding mock object as input to your test method. The order of the  objects are from in to out. So:</p> <pre><code>import unittest\nfrom unittest import mock\n\n\nclass TestYourClass(unittest.TestCase):\n    @mock.patch(\"path.to.your.module.some_other_import\")\n    @mock.patch(\"path.to.your.module.import_you_want_to_mock\")\n    def test_your_method(self, mock_some_import, mock_some_other_import):\n        ...\n</code></pre> <p>Then in the method itself you can take that mock object and define a different return value. <pre><code>mock_some_import.return_value = \"some_other_output\"\n</code></pre></p> <p>You can even go deeper in a structure, so: <pre><code>mock_some_import.return_value.some_other_object.return_value = \"some_other_output\"\n</code></pre> In the examples you see <code>return_value</code>, this just indicates that a function or method gets called, so just <code>()</code>. So without the <code>return_value</code> you are referring to the function or object itself. Meaning that you can replace  a function or object rather than just the output.</p> <p>Important</p> <p>It is important to realise that when you import packages in python, it gets registered to an import table. The patching will just replace the original entry of the import with a <code>MagicMock</code> object. So you can  mock all the things that you import.</p>"},{"location":"coding/Python/unittest/#mocking-built-ins","title":"Mocking built-ins","text":"<p>Built-ins are immutables and therefore can not be patched in the same way. To resolve this we can wrap the built-in module. See this thread: https://stackoverflow.com/a/55187924/.</p> <p>An example could be the builtin <code>datetime.today()</code>. </p> <p>From the stackoverflow answer (link)</p> <pre><code>from unittest import mock, TestCase\n\nimport foo_module\n\nclass FooTest(TestCase):\n\n    @mock.patch(f'{foo_module.__name__}.datetime', wraps=datetime)\n    def test_something(self, mock_datetime):\n        # mock only datetime.date.today()\n        mock_datetime.date.today.return_value = datetime.date(2019, 3, 15)\n        # other calls to datetime functions will be forwarded to original datetime\n</code></pre> <p>With wraps you just put a wrapper around the datetime module. All the function calls get forwarded. Thus when you  do not define another return value, it will just forward the call to the original function and so all the functions you do not change will just function normally.</p>"},{"location":"coding/Python/unittest/#running-unittests","title":"Running unittests","text":"<pre><code>python -m unittest path/to/your/tests/or_test_file.py\n</code></pre>"},{"location":"coding/Python/unittest/#pytest","title":"Pytest","text":"<p>With the <code>Pytest</code> library you can orchestrate and automate your tests in a more sophisticated way. You can run  tests defined using the <code>unittest</code> library with pytest.</p> <p>I have not worked with pytest in a pytest way yet, with for examples <code>fixtures</code>, to be able to say something about it now. More will follow.</p>"},{"location":"coding/Python/unittest/#running-tests-using-pytest","title":"Running tests using pytest","text":"<pre><code>pytest path/to/your/tests/or_test_file.py\n</code></pre>"},{"location":"coding/Python/unittest/#coverage-report-generation","title":"Coverage Report Generation","text":"<p>For the coverage we are using the coverage package, which can be pip-installed. <pre><code>pip install coverage\n</code></pre></p> <p>Then you can run the tests via coverage. In this example we use pytest to run the tests. <pre><code>coverage run -m pytest path/to/your/test_file.py\n</code></pre></p> <p>After the tests are run, you can let <code>coverage</code> generate a summary in the command line, or output an extensive report in html. In this html report you not only see the coverage percentage per file, but you can also drill-through to see the code with the parts not covered highlighted in red.</p> <p>Create a simple report in the terminal: <pre><code>coverage report --include=\"./path/to/your/modules/*.py\"\n</code></pre> Create a html reportL <pre><code>coverage html --include=\"./path/to/your/modules/*.py\"\n</code></pre></p> <p>By defining <code>--include</code>, you can limit the report to a specific part of your code base. This will speed up the  generation and prevent unwanted codes to be included.</p>"},{"location":"coding/Python/IDE/pycharm/","title":"Pycharm","text":"<p>Pycharm is created by Jetbrains, which also developed Intellij.</p>"},{"location":"coding/Python/IDE/pycharm/#configure-default-test-runner","title":"Configure default test runner","text":"<p>Go to <code>File</code> then <code>Settings</code>.</p> <p></p> <p>Then in the new window navigate to <code>Tools</code> -&gt; <code>Python Integrated Tools</code> </p> <p>From the dropdown select the test runner of your choosing.</p>"},{"location":"coding/Python/useful_libraries/boxsdk/","title":"BoxSDK","text":""},{"location":"coding/Python/useful_libraries/boxsdk/#setting-up-a-box-app","title":"Setting up a Box app","text":"<ol> <li> <p>Go to https://app.box.com/developers/console and create a new app.</p> <p></p> </li> <li> <p>Choose custom app:</p> <p></p> </li> <li> <p>Go for the option with JWT and choose your app name.</p> <p></p> </li> <li> <p>Once in your app, navigate to the configurations tab.</p> <p></p> </li> <li> <p>The only permissions we need for a simple landing zone app are the read all files and folders and write all files and folders.</p> <p></p> </li> <li> <p>Now scroll down and generate a public/private key pair. The configuration file will be automatically downloaded. Please do not lose the file, otherwise you will need to generate a new key pair!</p> <p></p> </li> <li> <p>The config file will look like:</p> <pre><code>    Configuration file\n    {\n      \"boxAppSettings\": {\n        \"clientID\": \"68zmu8tw4xaax0f4y1mjz5qqukx5v0u3\",\n        \"clientSecret\": \"********************************\",\n        \"appAuth\": {\n          \"publicKeyID\": \"\",\n          \"privateKey\": \"\",\n          \"passphrase\": \"\"\n        }\n      },\n      \"enterpriseID\": \"737833\"\n    }\n</code></pre> </li> <li> <p>In order to authenticate through this config file, we need to get the app authorized by the Box Admins.  For this Navigate to Authorization and press submit. In that development environment you are admin yourself  so you can approve yourself.</p> <p>    The Box admins will see your app pop up in their list with apps, and they can then authorize it.</p> <p></p> </li> <li> <p>In order to access the app programmatically we will use python and the boxsdk for python.</p> <pre><code>  pip install boxsdk[jwt]\n</code></pre> </li> <li> <p>Making connection to Box.</p> <pre><code>from boxsdk import JWTAuth\nfrom boxsdk import Client\nfrom boxsdk.object.collaboration import CollaborationRole\n\nconfig_file_path = './819136809_sh9k5jf3_config.json'\nauth = JWTAuth.from_settings_file(config_file_path)\nclient = Client(auth)\n\n# Print the user info, just a check to confirm the connection.\nprint(client.user().get())\n</code></pre> </li> <li> <p>Now we are logged in as the service user. You won't be seeing any files from your own box. To check that we can request all items in the root folder:     <pre><code># Folder_id 0 is standard for the root folder. As it is generic, we cannot grant permissions on this folder.\nfolder_contents = client.folder(folder_id='0').get_items()\n\nfor item in folder_contents:\n    print(item)\n</code></pre></p> </li> <li> <p>The final step is to create a subfolder in the root folder of the service user and make ourselves co-owner. We should create a subfolder for each solution we are working on.</p> <pre><code># Folder_id 0 is standard for the root folder. As it is generic, we cannot grant permissions on this folder.\nroot_folder = client.folder(folder_id='0')\n\ncollab_folder = root_folder.create_subfolder('LandingZone')\ncollaborator_email = '&lt;Put your own email here&gt;'\ncollab_folder.add_collaborator(collaborator_email, CollaborationRole.CO_OWNER)\n</code></pre> </li> <li> <p>With the permission given to ourselves, we should be able to see this folder in our own root directory.</p> <p>   And when we click on the folder and look at the details pane on the right, we will see that the app is listed as the owner of the folder:</p> <p></p> </li> </ol>"},{"location":"coding/Python/useful_libraries/sqlfluff/","title":"SQLFluff","text":"<p>SQLFluff is a tool to lint your SQL code. It can detect improvements in the SQL and you can also let it change the SQL code. In order for SQLFluff to detect improvements, it first needs to disect the SQL queries and classify each of the elements in the query. This will result in a abstract syntax tree for all the sql nodes. Because SQLFluff first does that, we can make use of that for our own use-cases as well.</p> <p>To install sqlfluff we can just use pip: <pre><code>pip install slqlfuff\n</code></pre></p>"},{"location":"coding/Python/useful_libraries/sqlfluff/#intended-use","title":"Intended use","text":"<p>This is just using the command line to tell sqlfluff to check or correct one or more sql files. We will not go into detail (for now). The documentation should give enough handles to start using the tool.</p>"},{"location":"coding/Python/useful_libraries/sqlfluff/#use-sqlfluff-to-our-advantage","title":"Use SQLFluff to our advantage","text":"<p>The standard simple API of SQLFluff return dictionaries of the tree structure. This is not practical, as we can not make use of the built-in helper functions. So what we want is to have the native SQLFluff objects.</p> <p>First let's import the Linter: <pre><code>from sqlfluff.core import Linter\n</code></pre></p> <p>Then we need to initialise the linter. The simplest configuration we can pass is which dialect we want to parse. So for example:</p> <pre><code>linter = Linter(dialect=\"databricks\")\n</code></pre> <p>Then we can use the <code>parse_string</code> method to parse a query.</p> <pre><code>parsed_string = linter.parse_string(\"SELECT * FROM some_schema.some_table as t WHERE t.id = 1\")\n</code></pre> <p>The output is a <code>ParsedString</code> object, which contains info on whether there were errors encountered while parsing the query, how long t took and most importantly, the parsed SQL tree. The tree can be accessed by:</p> <pre><code>parsed_tree = parsed_string.tree\n</code></pre>"},{"location":"coding/Python/useful_libraries/sqlfluff/#manually-navigating-the-tree","title":"Manually navigating the tree","text":"<p>You can then navigate the tree manually by requesting the segments with: <pre><code>parsed_tree.segments[0].segments[0].segments\n</code></pre></p> <p>This will result in the following output: <pre><code>Out[8]: (&lt;SelectClauseSegment: ([L:  1, P:  1])&gt;,\n &lt;WhitespaceSegment: ([L:  1, P:  9]) ' '&gt;,\n &lt;Dedent: (None) ''&gt;,\n &lt;FromClauseSegment: ([L:  1, P: 10])&gt;,\n &lt;WhitespaceSegment: ([L:  1, P: 42]) ' '&gt;,\n &lt;WhereClauseSegment: ([L:  1, P: 43])&gt;)\n</code></pre></p>"},{"location":"coding/Python/useful_libraries/sqlfluff/#select-deeper-segment-based-on-type","title":"Select deeper segment based on type","text":"<p>Instead of getting the segment by passing an index, we can also use the <code>get_child</code> or  <code>get_children</code> methods.</p> <pre><code>parsed_tree.segments[0].segments[0].get_child(\"from_clause\")\n</code></pre> <p>Output: <pre><code>Out[12]: &lt;FromClauseSegment: ([L:  1, P: 10])&gt;\n</code></pre></p> <p>To check what the contents are of a segment, we can use <code>raw</code>: <pre><code>parsed_tree.segments[0].segments[0].get_child(\"from_clause\").raw\n</code></pre></p> <p>Output: <pre><code>Out[13]: 'FROM some_schema.some_table as t'\n</code></pre></p>"},{"location":"coding/Python/useful_libraries/sqlfluff/#automatic-crawl-for-a-segment-type","title":"Automatic crawl for a segment type","text":"<p>Instead of manually going through the tree to access the desired segment, we can also make  use of the <code>recursive_crawl</code> method to do that for you. So to get to the from clause we can also use:</p> <p><pre><code>list(parsed_tree.recursive_crawl(\"from_clause\"))\n</code></pre> We use list here because the crawler returns a generator. The <code>recursive_crawl</code> method  can be tweaked to not recurse into a certain segment type by specifying <code>no_recursive_seg_type</code>. This can be useful if you don't want to go into selects inside of a join for example. Another useful parameter is the <code>recurse_into</code>, this is a boolean and when set to <code>False</code> it will not recurse deeper when it has found the segment you were looking for.</p>"},{"location":"coding/Python/useful_libraries/sqlfluff/#get-referenced-database-objects","title":"Get referenced database objects","text":"<p>If you just want to know which databa se objects are used in the query there is even a simpler method. You don't need to recursively crawl for all the elements of a  <code>FROM</code> or <code>JOIN</code>, because there is a built-in method for it.</p> <pre><code>parsed_tree.get_table_references()\n</code></pre> <p>Output: <pre><code>Out[17]: {'some_schema.some_table'}\n</code></pre></p> <p>Please note that the output is a set, so the contents are unique. However, if  the same table is called in the query with different upper or lower casing, then they will be handled as not the same.</p>"},{"location":"coding/commandline/bash/","title":"Useful bash snippets","text":""},{"location":"coding/commandline/bash/#split-string-by-newline","title":"Split string by newline","text":"<pre><code>readarray -t output_variable_name &lt;&lt;&lt;\"$variable_containing_string\"\n</code></pre>"},{"location":"coding/commandline/bash/#if-statement","title":"If statement","text":"<pre><code>if [ condition ]\nthen\n  do_something\nelse\n  do_something_else\nfi\n</code></pre> <p>It is important to have a space between the square brackets and the condition. Otherwise it will result in an error.</p>"},{"location":"coding/commandline/bash/#arrays","title":"Arrays","text":"<p>Array elements are accessed using curly brackets to indicate an array command and square brackets to indicate which index you want to access.</p> <p><pre><code>${array_variable[1]}\n</code></pre> Where the 1 indicates the second entry.</p> <p>You can also do a count on the array. <pre><code>${#array_variable[@]}\n</code></pre></p>"},{"location":"coding/commandline/bash/#parse-input-parameters","title":"Parse input parameters","text":"<pre><code>for i in \"$@\"; do\n  case $i in\n    -var1=*|--variable1=*)\n      VARIABLE1=\"${i#*=}\"\n      ;;\n    -vae2=*|--variable2=*)\n      VARIABLE2=\"${i#*=}\"\n      ;;\n    -*|--*)\n      echo \"Unknown option $i\"\n      exit 1\n      ;;\n    *)\n      ;;\n  esac\ndone\n</code></pre>"},{"location":"coding/commandline/git/","title":"Useful Git commands","text":""},{"location":"coding/commandline/git/#get-current-branch-name","title":"Get current branch name","text":"<pre><code>git rev-parse --abbrev-ref HEAD\n</code></pre>"},{"location":"coding/commandline/git/#git-rev-list","title":"git-rev-list","text":"<p>The <code>rev-list</code> command lists commit objects in reverse chronological order. For the full documentation please see: https://git-scm.com/docs/git-rev-list.</p> <p>Syntax: <pre><code>git rev-list [&lt;options&gt;] &lt;commit&gt;\u2026\u200b [--] [&lt;path&gt;\u2026\u200b]\n</code></pre></p> <p>Some handy options to consider:</p> <ul> <li><code>--count</code> counts the number of commits.</li> <li><code>--format</code> allows for formatting the output.</li> <li><code>--merges</code> only show merge commits.</li> </ul> <p>In the next subsections we will give some examples on handy use-cases.</p>"},{"location":"coding/commandline/git/#amount-of-commits-behind","title":"Amount of commits behind","text":"<pre><code>git rev-list --count &lt;current branch&gt;..&lt;target branch&gt;\n</code></pre>"},{"location":"coding/commandline/git/#amount-of-commits-ahead","title":"Amount of commits ahead","text":"<pre><code>git rev-list --count &lt;target branch&gt;..&lt;current branch&gt;\n</code></pre>"},{"location":"coding/commandline/git/#get-list-of-merge-commits","title":"Get list of merge commits","text":"<p>Let's say we have a <code>development</code> branch in which all commits are collected before going to production. Once we go to production the changes will be merged into the <code>master</code> branch. If you want to have a quick overview of which Pull Request are included you can use the  option <code>--merges</code>.</p> <pre><code>git rev-list --format=\"%Bauthor: %an\" --merges master..development\n</code></pre> <p>This however, will also print out the commit hash, related work items and the comment of the  commit. If we just wanted a short overview of what the title was and who merged it, we can pass the output to <code>grep</code> and look for <code>Merged PR</code>, as that is the default commit title when completing a Pull Request in Azure DevOps. If we also want to include the author, we can add that as well.</p> <p>More on grep.</p> <pre><code>git rev-list --format=\"%Bauthor: %an\" --merges master..development | grep -e \"Merged PR\" -e \"author: \"\n</code></pre>"},{"location":"coding/commandline/git/#changed-files","title":"Changed files","text":"<pre><code>git diff &lt;target branch&gt; --name-only\n</code></pre>"},{"location":"coding/commandline/grep/","title":"GREP","text":"<p>The grep (Global Regular Expression Print) utility searches a file for a pattern.  This pattern is a regular expression.</p> <p>For the full syntax documentation please refer to https://pubs.opengroup.org/onlinepubs/9699919799/utilities/grep.html</p>"},{"location":"coding/general/regex/","title":"Regular Expressions","text":"<p>Regular expressions (or in short Regex) are used to match a certain pattern in a text. It is also possible to substitute parts of a string. A great resource to learn and experiment with regex is regex101.</p> <p></p> <p>Regex101 provides among many things an interactive editor to try out your regular expressions. It also highlights parts of your regex and provides explanations on each of the parts.</p>"},{"location":"coding/general/regex/#pattern-matching","title":"Pattern matching","text":"<p>In this section we will go over some patterns that might be useful.</p>"},{"location":"coding/general/regex/#extract-string-between-some-qualifiers","title":"Extract string between some qualifiers","text":"<p>Let's say we have a weird structure of symbols enclosing a string. Starting with <code>!@#</code> and ending with <code>%^&amp;</code>.</p> <p><pre><code>!@# Some text between qualifiers %^&amp;\n\n!@# Some other text between qualifiers %^&amp;\n</code></pre> Then we can simply do the following: <pre><code>(?&lt;=!@#).*(?=%\\^&amp;)\n</code></pre></p> <p><code>(?&lt;=!@#)</code> means that there is something in front of the string you want to look for that contains those 3 symbols. <code>(?=%\\^&amp;)</code> means that there is something after the string that has the other 3 symbols. These are called  positive lookbehind and positive lookahead. Positive, because it indicates it should match the provided pattern.  The lookbehind and lookahead are not part of the match. </p> <p>The center part <code>.*</code> means that it should match any character except for line terminators. So for this example it works fine.</p> <p>But what if we have the following example.</p> <pre><code>!@# Some text\non multiple\nlines%^&amp;\n\n!@# Some other text\non multiple\nlines%^&amp;\n</code></pre> <p>For this to work we need to modify the center part to match absolutely everything.</p> <p><pre><code>(?&lt;=!@#)[\\w\\W]*(?=%\\^&amp;)\n</code></pre> We have replaced the dot with <code>[\\w\\W]</code>. The brackets mean a single character, <code>\\w</code> means all word characters and <code>\\W</code> means everything BUT a word character. By using this trick you are basically saying that you want to match everything. This  however does not work as intended yet. The matched text is:</p> <pre><code> Some text\non multiple\nlines%^&amp;\n\n!@# Some other text\non multiple\nlines\n</code></pre> <p>This is because the <code>*</code> is greedy. This means that it does not stop at the first <code>$%^</code>, but goes on until it hits the last. In order to make it lazy, we just need to put a <code>?</code> after the <code>*</code>.</p> <pre><code>(?&lt;=!@#)[\\w\\W]*?(?=%\\^&amp;)\n</code></pre> <p></p>"},{"location":"coding/general/regex/#substitution","title":"Substitution","text":"<p>With substitution you can replace what you have matched. </p>"},{"location":"coding/general/regex/#example-by-using-groups","title":"Example by using groups","text":"<p>Let's use the same example as for the qualifiers.</p> <pre><code>!@# Some text between qualifiers %^&amp;\n\n!@# Some other text between qualifiers %^&amp;\n</code></pre> <p>Say we want to remove the qualifiers but keep what is inside. We can then add brackets around the center part of the  regex to create a group. This group will get assigned an id, which we will use in the substitution. We also need to drop the lookbehind and lookahead, because they don't show up in the match, and thus will not be touched by the substitution.</p> <pre><code>!@#(.*)%\\^&amp;\n</code></pre> <p>Then in the substitution pattern we can refer to this group by <code>\\1</code>. In this case it is 1 because it is the first group in the regex. Because the group is all we want we the substitution pattern is just</p> <p><pre><code>\\1\n</code></pre> </p> Python <pre><code>import re\nstring_to_sub = \"!@# Some text between qualifiers %^&amp;\" \nsubbed_string = re.sub(r\"!@#(.*)%\\^&amp;\",r\"\\1\", string_to_sub) \n</code></pre>"},{"location":"coding/general/ssl-certificates/","title":"SSL-Certificates","text":""},{"location":"coding/general/ssl-certificates/#windows","title":"Windows","text":"<p>Windows has system certificates which other applications can make use of. This is not always configured correctly, which might lead to errors.</p> <p>In the next sections the solution is provided for several applications.</p>"},{"location":"coding/general/ssl-certificates/#python","title":"Python","text":"<p>You can install the following package (source: https://stackoverflow.com/a/57053415):</p> <pre><code>pip install pip-system-certs\n</code></pre>"},{"location":"coding/general/ssl-certificates/#git","title":"Git","text":"<p>You can tell Git to use schannel. (source: https://stackoverflow.com/a/48212753)</p> <pre><code>git config --global http.sslBackend schannel\n</code></pre>"},{"location":"ideas_bucket/","title":"Ideas bucket","text":"<p>This is a coat hanger where rough ideas can be stored as a separate page to be worked out in the future.  It is to prevent good ideas for a documentation page to go forgotten and wasted.</p>"}]}